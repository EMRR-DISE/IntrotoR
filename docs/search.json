[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R Training - July 2025",
    "section": "",
    "text": "1 Intro to R for DWR\nWelcome to the Intro to R class for the Department of Water Resources, 2024!\nFor the first time, this is not taught by outside consultants, but by your own colleagues here at the department.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R for DWR</span>"
    ]
  },
  {
    "objectID": "index.html#ground-rules",
    "href": "index.html#ground-rules",
    "title": "Introduction to R Training - July 2025",
    "section": "1.1 Ground rules",
    "text": "1.1 Ground rules\n\nPlease participate fully in the class. You will get out of this class exactly as much as you put into it. This means not multi-tasking, completing the exercises, and asking questions when you get stuck.\nPlease come prepared. Install software ahead of time, have the link to this book on-hand, and review materials from the previous day before the next day’s class.\nDon’t be afraid to ask questions! Any coding language can be frustrating, so please speak up if you are having trouble. One of our teaching assistants can work with you 1:1 for major problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R for DWR</span>"
    ]
  },
  {
    "objectID": "index.html#before-you-come-to-class",
    "href": "index.html#before-you-come-to-class",
    "title": "Introduction to R Training - July 2025",
    "section": "1.2 Before you come to class",
    "text": "1.2 Before you come to class\n\nInstall R\n\nGo to the Comprehensive R Archive Network (CRAN) webpage and download R for Windows.\nOpen the installer and follow the prompts to install R on your computer using the default settings. You may need an administrator account to complete this step.\nIf you already have R on your computer, please make sure it is version 4.0 or above. (The latest version is 4.4.1)\nSee full instructions here: https://rstudio-education.github.io/hopr/starting.html\n\nInstall R Studio\n\nGo to the Download Rstudio page on the Posit website and download R Studio for Windows.\nOpen the installer and follow the prompts to install R on your computer using the default settings. You may need an administrator account to complete this step.\n\nOpen RStudio and make sure it runs.\n\nThe window titled “Console” should come up and have the R version and some boilerplate information about R in it, with a blue carrot and cursor below it.\n\n\n\n\n\nInstall the tidyverse and here packages\n\nIn your console, type install.packages(\"tidyverse\", \"here\")\nThe package should download and install automatically. You do not need administrative privileges for this step.\nNOTE: There are sometimes issues accessing CRAN from within RStudio while connected to a DWR network. If you receive this message, use your home wifi or a cell phone hot spot to connect to the internet, then try again.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R for DWR</span>"
    ]
  },
  {
    "objectID": "index.html#meet-your-instructors",
    "href": "index.html#meet-your-instructors",
    "title": "Introduction to R Training - July 2025",
    "section": "1.3 Meet your instructors",
    "text": "1.3 Meet your instructors\n\nRosie Hartman (She/Her) is an Environmental Program Manager in the Division of Integrated Science and Engineering. She is an aquatic ecologist by training, with an emphasis on data synthesis, statistics, and data integration. She would love to tell you how to analyze your data.\nPerry (They/Them) is an Environmental Scientist with the Environmental Monitoring Program who is also getting a master’s degree in statistics from UC Davis. They are fluent in both Python and R.\nNick Rasmussen (He/Him) is a Senior Environmental Scientist with broad interests in aquatic ecology who has worked on projects ranging from the Salinity Control Gates to aquatic weeds.\nDave Bosworth (He/Him) is a Senior Environmental Scientist who comes from a water quality and contaminants background but has found his calling as a data scientist. He is passionate about making your code more efficient and automating all the boring stuff.\nTed Swift (He/Him) is a Senior Environmental Scientist with the Quality Management Section who is working to increase the number of data science training available to DWR scientists.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R for DWR</span>"
    ]
  },
  {
    "objectID": "code/Day1_01_Intro.html",
    "href": "code/Day1_01_Intro.html",
    "title": "Day 1 - An introduction to R",
    "section": "",
    "text": "R\nR is a flexible coding language that anyone can learn\nI am taking a lot of this tutorial from the e-book “Hands on Programming in R” https://rstudio-education.github.io/hopr/\nWe’re setting this up as an online web-book where there are chucks of code that you can copy and paste into your console by clicking on the clipboard icon in the top right. For the exercises, we have hidden the answers but you can check yourself once you have tried it.\nIsn’t R a language?\nYou may hear me speak of R in the third person. For example, I might say, “Tell R to do this” or “Tell R to do that”, but of course R can’t do anything; it is just a language. This way of speaking is shorthand for saying, “Tell your computer to do this by writing a command in the R language at the command line of your RStudio console.” Your computer, and not R, does the actual work.\nIs this shorthand confusing and slightly lazy to use? Yes. Do a lot of people use it? Everyone I know–probably because it is so convenient.\nWait… am I using R or Rstudio?\nR is a language. You can use a console by itself to run code, but it is a lot easier to use a console together with a text editor, file explorer, image viewer, and other tools. That’s where RStudio comes in.\nRstudio is an integrated development environment that includes a debugger, version control, help section, and lots of other nifty tools that make coding easier. (you’ll get a tour in a minute)",
    "crumbs": [
      "Day 1 - An introduction to R"
    ]
  },
  {
    "objectID": "code/Day1_01_Intro.html#why-use-code",
    "href": "code/Day1_01_Intro.html#why-use-code",
    "title": "Day 1 - An introduction to R",
    "section": "Why use code?",
    "text": "Why use code?\nThere is a very steep learning curve to R (or any coding language). You are likely to get frustrated when code doesn’t work. The first things you learn are probably things you could do easier and faster in Excel. So why bother?\n\nReproducibility. Your workflow will be documented and can be repeated quickly and easily. It is also easier to fix your mistakes because you can see exactly what you did in every step of the process!\nLarge datasets. The larger your dataset is, the more difficult it will be to analyze in Excel or other GUI-based programs.\nAdvanced statistical capabilities. You can go a lot further with tools like mixed models, multivariate analyses, machine learning, and population models that are impossible in Excel.\nHuge crowd-sourced base of packages and help. The R community is really what makes it special. Pretty much anything you want to do has probably been done before, and there is a publicly-available package to help you out.",
    "crumbs": [
      "Day 1 - An introduction to R"
    ]
  },
  {
    "objectID": "code/Day1_01_Intro.html#the-very-basics",
    "href": "code/Day1_01_Intro.html#the-very-basics",
    "title": "Day 1 - An introduction to R",
    "section": "The Very basics",
    "text": "The Very basics\nOK, everyone, open Rstudio. Click on the new project button. A ‘project’ is basically an organized directory that has all of the data, code, and outputs for a particular work product. It’s especially useful once you start working on multiple files and reading in and out data and stuff, so it’s good to get in the habit of always working in a project. For now, choose a location for a folder that will contain all the work you want to do for this class. This folder is your “working directory”, meaning, the default location for loading inputs and writing outputs, scripts, etc.\n\nNow let’s take a look at your RStudio Screen. Go over the different panes and how to navigate and customize.\nThis should be in your console:\n\nNow go to File –&gt; new file –&gt; R script.\nThis will be your first R script. You will type things in the script, then transfer them to the console to run them. You can technically type directly into the console, but then you can’t save them, and that’s bad, so get into the habit of writing everything in your script.\nComments\nAnother good habit to start early is commenting your code. Comments are indicated by hashtags #. They are parts of your script that don’t do anything, just give you information about what you intended in your code and why you did what you did. RStudio helpfully color-codes them for you.\nComments are extra useful if you are sharing code with someone else. And remember “Someone else” might just be you in the future!\nFor example:\n\n#This is my first R script\n\n#test out addition\n1+1\n\n[1] 2\n\n#use a function\nsum(c(1,1))\n\n[1] 2\n\n\nRunning code\nFrom your script, you can hit the ‘Run’ icon in the top right corner to move a line of code from your script to your console.\nYou can also hit ctrl+enter to do the same thing.\nHighlight multiple lines of code to run more than one thing at once.\nThe “source” button at the top of the script runs the entire script at once without printing all the output. This is useful of you have a script that is mostly setup stuff or homemade functions.\nYou will notice that in the console all the lines of code you run start with a &gt;. If you have multiple lines of code strung together, there will be + at the start of the new lines until the end of the code. In an R script, you can break your code up into lines to make it all fit on your screen, so long as the parentheses line up.\nThe output does not start with a carrot or plus sign.\nExercise\nNow your turn!\n\nUse comments to write a title, date, and your name at the top of the script.\nSave your script in your project working directory\nType 24/2 in your script\nRun the script\n\nClick below for the answer when you are done!\n\nCode# My First R script\n#\n\n24/2",
    "crumbs": [
      "Day 1 - An introduction to R"
    ]
  },
  {
    "objectID": "code/Day1_02_Objects.html",
    "href": "code/Day1_02_Objects.html",
    "title": "\n2  Objects and Functions\n",
    "section": "",
    "text": "2.1 Objects and Assignment\nThe joy of coding is that you can quickly to a lot of things at once. So, if I had a dataset with temperatures in Fahrenheit that I want to convert to Celsius, we can use R to do that all at once.\nLet’s say our list of temperatures are: 72, 69, 57, 58, 71, 64, 65, 70, 59\nWe can use R like a calculator to convert each of these numbers\n(72-32)*5/9\n\n[1] 22.22222\n\n(69-32)*5/9\n\n[1] 20.55556\n\n(57-32)*5/9\n\n[1] 13.88889\n\n(58-32)*5/9\n\n[1] 14.44444\n\n(71-32)*5/9\n\n[1] 21.66667\n\n(64-32)*5/9\n\n[1] 17.77778\n\n(65-32)*5/9\n\n[1] 18.33333\n\n(70-32)*5/9\n\n[1] 21.11111\n\n(59-32)*5/9\n\n[1] 15\nBut that’s tedious. Instead, we can clump these numbers together into a vector and assign it to a variable.\nLet’s call our vector “temps”\ntemps &lt;- c(72, 69, 57, 58, 71, 64, 65, 70, 59)\n\ntemps\n\n[1] 72 69 57 58 71 64 65 70 59\nThe &lt;- is called the “assignment operator”. You can also use = to do the same thing. The c is short for “concatenate”, which means “stick all these things together”. We now have an object temps that is a vector of values. Type “temps” into your console.\nWe can then perform operations on the whole vector of values at once. For example\n(temps-32)*5/9\n\n[1] 22.22222 20.55556 13.88889 14.44444 21.66667 17.77778 18.33333 21.11111\n[9] 15.00000\nIf we want to save that output, we need to assign it to a new variable\ntemps_C &lt;- (temps-32)*5/9\nNote that this doesn’t give you any output, it just assigns a value to temps_C. If we want to see what temps_C is, we need to print it out.\ntemps_C\n\n[1] 22.22222 20.55556 13.88889 14.44444 21.66667 17.77778 18.33333 21.11111\n[9] 15.00000\nIf you want to print it as you make the assignment, you can put parentheses around it.\n(temps_C2 &lt;- temps_C*2)\n\n[1] 44.44444 41.11111 27.77778 28.88889 43.33333 35.55556 36.66667 42.22222\n[9] 30.00000\nYou’ll also notice that in the Environment tab in your Rstudio window you should now have temps and temps_C. You can click on them to see them.",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objects and Functions</span>"
    ]
  },
  {
    "objectID": "code/Day1_02_Objects.html#objects-and-assignment",
    "href": "code/Day1_02_Objects.html#objects-and-assignment",
    "title": "\n2  Objects and Functions\n",
    "section": "",
    "text": "2.1.1 Exercise\nConvert these numbers from miles per hour to meters per second. (use assignment)\n21, 25, 100, 50, 36, 72, 15\nClick below for the answer when you are done.\n\nCode#the variable \"mph\" will be assigned the value of a vector of all our numbers\nmph &lt;- c(21, 25, 100, 50, 36, 72, 15)\n\n#to convert mph to mps, divide by 2.237\nmps &lt;- mph/2.237\n\n#now print the output\nmps\n\n\n\n2.1.2 Type of objects\nWe can call temps and temps_C objects. Specifically, they are numeric vectors. Let’s go through some different types of objects.\n\nScalars - single value\nVectors - list if values (one dimensional), all the same data class\nMatrices - values in rows and columns, all the same class (two dimensional)\nArrays - multiple matrices stacked up (three-dimensional)\nData Frames - Data of different types in rows and columns, where all columns are the same length and all rows are the same width. Data in the different columns can be of different classes.\nLists - data of different types and different lengths in rows and columns.\n\n2.1.3 Data classes\n\nnumeric - Numbers (duh), these can be integers, real numbers, etc.\ncharacter - Any sequence of letters and numbers or special characters.\nlogical - TRUE/FALSE (always in all caps, or abbreviated T and F)\nfactor - Categorical variables that take a limited set of values. May be ordered (like water year types), or unordered (like families of fishes)\ndate/time classes (giant can of worms, we’ll talk about those later)\n\nUsing class and str to get info about objects. also View and head.\n\nfoo &lt;- \"Cat\"\ndog &lt;- 24\n\nclass(foo)\n\n[1] \"character\"\n\nstr(foo)\n\n chr \"Cat\"\n\nclass(dog)\n\n[1] \"numeric\"\n\nfoo2 = c(\"Mouse\", \"Cat\", \"dog\", \"Squirrel\")\nstr(foo2)\n\n chr [1:4] \"Mouse\" \"Cat\" \"dog\" \"Squirrel\"\n\n\n\n2.1.4 Functions\nFunctions are little sequences of code that do something useful. There are lots of built-in functions, plus you can define your own functions when you get a little more practice.\nThe basic structure is:\nfunction(arguments)\nwhere “arguments” are the inputs to your function.\nThe parentheses are the “trigger” that tells R to run the function. Typing the name of the function without the parentheses prints the code in the console. And you can write your own when you get good.\n\n#when we calculated the sum of 1 +1, that was a function\n\nsum(1+1)\n\n[1] 2\n\nsum\n\nfunction (..., na.rm = FALSE)  .Primitive(\"sum\")\n\n#mean is another useful function\nmean(c(1,2,3,4))\n\n[1] 2.5\n\n\nNotice that as you start typing the function, a box pops up in RStudio that prompts you with the arguments you might want to use. Also note that we have to “feed” the mean function a vector of values with c (concatenate) in front of it.\nYou can convert between data classes using as.class . This is particularly useful for factors. If you have a bunch of character strings, R automatically puts them in alphabetical order, but if you have a factor, you can specify which order the different factor levels should go in.\n\ndog2 &lt;- as.character(dog)\ndog2\n\n[1] \"24\"\n\nclass(dog2)\n\n[1] \"character\"\n\n# This only works if things are compatible\nas.numeric(foo)\n\n[1] NA\n\nas.numeric(dog2)\n\n[1] 24\n\n#let's look at factors\nfoo3 = as.factor(foo2)\nstr(foo3)\n\n Factor w/ 4 levels \"Cat\",\"dog\",\"Mouse\",..: 3 1 2 4\n\n#You can specify factor order\nfoo4 &lt;- factor(foo2, levels = c(\"Squirrel\",\"Mouse\", \"Cat\", \"dog\"))\nstr(foo4)\n\n Factor w/ 4 levels \"Squirrel\",\"Mouse\",..: 2 3 4 1\n\n#turn it baack into a character\nas.character(foo4)\n\n[1] \"Mouse\"    \"Cat\"      \"dog\"      \"Squirrel\"\n\n#factors are internally stored as numbers, so you can even convert to numeric\nas.numeric(foo4)\n\n[1] 2 3 4 1\n\n#you can change the labels\nfoo5 &lt;- factor(foo2, levels = c(\"Squirrel\",\"Mouse\", \"Cat\", \"dog\"),\n               labels = c(\"Wet\", \"Normal\", \"Dry\", \"Critical\"))\nstr(foo5)\n\n Factor w/ 4 levels \"Wet\",\"Normal\",..: 2 3 4 1\n\n\nYou can even write your own functions! This often intimidates people at first, but it can be really useful as you get more advanced, since it is a “short cut” that means you can get more done with less typing.\nFor example, if you wanted to convert a bunch of numbers from Farenheit to Celcius (like we did above), you can write a function that takes a number, subtracts 32, and multiplies by 5/9 all at once.\n\n#we'll name our function FtoC\n#it takes a single argument - temperatur ein Farenheit, which we'll call TempF\n\nFtoC &lt;- function(TempF) {\n  C &lt;- (TempF-32)*5/9\n  return(C) #tell the function what to give you back\n}\n\nFtoC(32)\n\n[1] 0\n\nFtoC(c(23,24,56,101))\n\n[1] -5.000000 -4.444444 13.333333 38.333333\n\nFtoC(temps)\n\n[1] 22.22222 20.55556 13.88889 14.44444 21.66667 17.77778 18.33333 21.11111\n[9] 15.00000\n\n\nDon’t worry about writing your own functions too much right now, but I want you to know it’s a possibility.\n\n2.1.4.1 Exercise\nSee if you can use use these functions\n\nmean - calculate the mean of 23, 24, 15, 12, 53, 23, 1, 45\n\nsum - calculate the sum of all numbers below 50\n\nHint! You can ask for a sequence of numbers with a : between the first and last number.\n\n\nabs (absolute value)\nlog - default is ln, not log10\nround - can specify decimal places\n\nexp (exponent)\nClick below for the answers when you’re done!\n\n\n\nCodemean(c(23, 24, 15, 12, 53, 23, 1, 45))\n\nsum(c(1:50))\n\nabs(c(-1, 2, -10, 5))\n\n#natural log\nlog(100)\n\n#log base 10\nlog(100, base = 10)\n\n#the default for round is no decimal points\nround(5.345673)\n\n#but you can specify the number of decimal points with the second argument\nround(5.345673, digits = 2)\n\n#this is the same as e^20\nexp(20)\n\nexp(log(10))",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objects and Functions</span>"
    ]
  },
  {
    "objectID": "code/Day1_03_Packages.html",
    "href": "code/Day1_03_Packages.html",
    "title": "\n3  Packages and importing data\n",
    "section": "",
    "text": "3.1 Packages\nThe real benefit of R is that it is open-source, and tons and tons of people have developed ‘expansion packs’ for R. You can go a very long way with just the built-in R functions, but many people have developed slightly different ways of doing things, easier methods, and more advanced things.\nLet’s go over to the R website and talk packages - https://www.r-project.org/\nWe had everyone install the tidyverse packages before getting started. This is actually a set of packages that all work together to make code a little more intuitive. Let’s go over to the “Packages” tab in RStudio and check them out.\nYou’ll notice that besides the tidyverse, there are a number of other packages in this tab that you didn’t install - they came along with base R.\nWhen you want to install or update packages, you can use the install.packages command, or the GUI in RStudio. This command reaches out to the CRAN website and downloads the code files, saving them to your “library”. You only have to do this once. However, at the start of every R session you will need to load the package into your environment using the library command. This is usually done at the top of your script.\n#load required libraries\nlibrary(readr)\nLet’s check out the documentation\n#check out documentation\n?readr\nClick on the index, then one of the vignettes - those are very useful!",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Packages and importing data</span>"
    ]
  },
  {
    "objectID": "code/Day1_03_Packages.html#packages",
    "href": "code/Day1_03_Packages.html#packages",
    "title": "\n3  Packages and importing data\n",
    "section": "",
    "text": "dplyr - data manipulation\nlubridate - dates and times\nggplot2 - graphics\ntidyr - more data manipulation\nforcats - working with categorical variables (factors)\nreadr - importing data from spreadsheets\nstringr - working with character strings\ntibble - nicer checking and formatting for tables and data frames\n\n\n\n\n\n\n\n\n\n3.1.1 Package conflicts\nAfter you’ve loaded a package, you might get some warnings about conflicted packages. These are different functions with the same name in two different packages. Mostly it isn’t a problem, but sometimes you’ll have to specify which function you mean.\nSpecify which you want with package::function\n\nlibrary(lubridate)\n\nIf you really want the base version instead of the one from a package, you can use the exclude argument.\n\n#remove the lubrdate library we just loaded\ndetach(\"package:lubridate\")\n\n#now reload with the exclusion\nlibrary(lubridate, exclude = \"date\")\n\n\n3.1.2 Exercise\nLet’s try using a function that is in a package. glimpse is an simple function that tells you about a data frame. R has a number of built-in data sets that you can play with, and one is mtcars. It’s just a table of different makes and models of cars and their stats.\n\n#The View function is built in. \nView(mtcars)\n\n\n#the \"glimpse\" function is in the dplyr package. It's part of the tidyverse set of packages. You should have installed it already\nglimpse(mtcars)\n\nError in glimpse(mtcars): could not find function \"glimpse\"\n\n\nEven though you installed it, you still need to load it into your workspace using the library command.\n\nlibrary(dplyr)\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n3.1.3 Exercise\nNow go to the documentation for dplyr and look through the “Introduction to dplyr” vignette. Take 10 mins and see if you try out some of the examples. We’ll be using a lot of these dplyr functions later in the class.\n\n?dplyr\n\nvignette(\"dplyr\")",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Packages and importing data</span>"
    ]
  },
  {
    "objectID": "code/Day1_03_Packages.html#importing-and-exporting-data",
    "href": "code/Day1_03_Packages.html#importing-and-exporting-data",
    "title": "\n3  Packages and importing data\n",
    "section": "\n3.2 Importing and Exporting data",
    "text": "3.2 Importing and Exporting data\nNext, we must import our data.\nThe common function for this is read_csv from readr (which is nested in tidyverse). For our demonstrations, we will use the WQ_P8D7.csv file housed in the data folder.\nWhen importing data, we must specify the filepath where it’s housed. There are multiple ways to do this. We could hard-code in the path, which is called the absolute path:\n\ndf_wq &lt;- read_csv('C:/R/IntrotoR/data/WQ_P8D7.csv')\n\nError: 'C:/R/IntrotoR/data/WQ_P8D7.csv' does not exist.\n\n\nHowever, this code is very specific and will break when used on other computers.\nIf we instead house data in a Project, we can make use of relative filepaths. These are ideal because anyone who uses the Project can run the code:\n\ndf_wq &lt;- read_csv('data/WQ_P8D7.csv')\n\nIf you received an error here, this is probably because you either didn’t save the file in the right folder, or you are not working in a project.\n\n\n\n\n\n\nData File Extensions and Delimiters\n\n\n\nHere, we used the read_csv function, which takes .csv files by default. But what is a csv?\n“csv” stands for “comma separated values”, where the comma is called a delimiter; it tells the code where to separate the data cells. If you want to use a different delimiter, you can use the read_delim function (also from the readr package):\n\nread_delim('data/delim_ex.txt', delim = '|') # data separated by |\n\n# A tibble: 2 × 4\n  col   headers are       first    \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n1 here  is      an        example  \n2 of    a       different delimiter\n\n\nfor tab delimited data (a fairly frequent format), there’s read_tsv:\n\nread_tsv('data/tab_ex.tsv')\n\n# A tibble: 2 × 4\n  col   headers are       first    \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n1 here  is      an        example  \n2 of    a       different delimiter\n\n\nExcel files (.xlsx) are unique because they’re not solely defined by their delimiters, which allows for more complicated file formatting. To import these, we use read_excel from the readxl package:\n\nlibrary(readxl)\n\nread_excel('data/excel_ex.xlsx', sheet = 'Sheet1') # read the first sheet (by name)\n\n# A tibble: 2 × 4\n  col   headers are   first  \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  \n1 here  is      an    example\n2 of    an      excel file   \n\nread_excel('data/excel_ex.xlsx', sheet = 2) # read the second sheet (by number)\n\n# A tibble: 2 × 4\n  col   headers are   first  \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  \n1 here  is      an    example\n2 of    excel   sheet 2      \n\n\n\n\n\nWe now have a data frame object called df_wq. We can use head to see what the first few rows of the data frame look like:\n\nhead(df_wq)\n\n# A tibble: 6 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5             98        0.15\n2 D7      2020-01-22  0.67       0.87            82        0.21\n3 P8      2020-02-14  1.46       0.69            81        0.25\n4 D7      2020-02-20  2.15       0.5             86        0.14\n5 P8      2020-03-03  1.4        0.56            80        0.11\n6 D7      2020-03-06  1.89       1.13            93        0.22\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\nAnd glimpse to see information about the columns:\n\nglimpse(df_wq)\n\nRows: 62\nColumns: 20\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2020-01-16, 2020-01-22, 2020-02-14, 2020-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.64, 0.67, 1.46, 2.15, 1.40, 1.89, 4.73, 1.74, 6.4…\n$ Pheophytin         &lt;dbl&gt; 0.50, 0.87, 0.69, 0.50, 0.56, 1.13, 1.25, 0.89, 0.8…\n$ TotAlkalinity      &lt;dbl&gt; 98.0, 82.0, 81.0, 86.0, 80.0, 93.0, 59.0, 78.0, 63.…\n$ DissAmmonia        &lt;dbl&gt; 0.150, 0.210, 0.250, 0.140, 0.110, 0.220, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 2.800, 0.490, 1.700, 0.480, 1.600, 0.380, 1.070, 0.…\n$ DOC                &lt;dbl&gt; 3.90, 0.27, 2.80, 0.39, 2.00, 0.19, 2.80, 1.20, 3.1…\n$ TOC                &lt;dbl&gt; 4.10, 0.32, 2.50, 0.41, 2.10, 0.20, 2.80, 1.20, 3.1…\n$ DON                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0.30, 0.20, 0.30, 0.10, 0.5…\n$ TotPhos            &lt;dbl&gt; 0.310, 0.082, 0.130, 0.130, 0.190, 0.100, 0.188, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.200, 0.071, 0.130, 0.065, 0.140, 0.082, 0.177, 0.…\n$ TDS                &lt;dbl&gt; 380, 9500, 340, 5800, 290, 8700, 280, 7760, 227, 11…\n$ TSS                &lt;dbl&gt; 8.9, 38.0, 2.2, 18.0, 1.4, 28.0, 6.6, 35.6, 5.3, 23…\n$ TKN                &lt;dbl&gt; 0.520, 0.480, 0.430, 0.250, 0.400, 0.200, 0.400, 0.…\n$ Depth              &lt;dbl&gt; 28.9, 18.8, 39.0, 7.1, 39.0, 7.2, 37.1, 5.2, 36.7, …\n$ Secchi             &lt;dbl&gt; 116, 30, 212, 52, 340, 48, 100, 40, 160, 44, 120, 6…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 3, 2, 3, 2, 4, 2, 3, 2, 2, 1, 1, …\n$ SpCndSurface       &lt;dbl&gt; 667, 15532, 647, 11369, 530, 16257, 503, 12946, 404…\n$ WTSurface          &lt;dbl&gt; 9.67, 9.97, 11.09, 12.51, 13.97, 13.81, 23.46, 21.1…",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Packages and importing data</span>"
    ]
  },
  {
    "objectID": "code/Day1_04_ErrorsHelp.html",
    "href": "code/Day1_04_ErrorsHelp.html",
    "title": "\n4  Errors and help\n",
    "section": "",
    "text": "4.1 Errors and Getting Help\nOne of the most frustrating parts of coding is errors. Your computer is very good at doing exactly what you tell it to, but that might not be what you want.\ndates = c(\"Monday\", \"Tuesday\", \"Wednesday\")\n\n#select the first element of the `dates` vector (more on this tomorrow)\ndate[1]\n\nError in date[1]: object of type 'closure' is not subsettable\nThis is one of my least favorite error messages. What the heck does that even mean?\nIt turns out we accidentally typed date instead of dates, and date is a reserved term in R, so it was trying to subset a word that designates a data type and everyone was confused. But don’t worry! It gets worse.\nEveryone has trouble interpreting error messages at first. Fortunately, error messages are improving, and you will get better at understanding them.\nAnother thing to know is that not all red text is errors. Some are warnings. Don’t worry about what this code does for now, just see what the output looks like.\nlibrary(ggplot2)\nmtcars[1,1] = NA \n\nggplot(mtcars, aes(x = mpg, y = hp))+ geom_point()+ geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nEverything still ran, but it gave to a warning to let you know something did not go as planned.\nWe also sometimes get messages, which are usually not even red.\n#don't worry about what this code does for now, just notice that we get a message telling you what it decided to use for the smoothing term.\nggplot(mtcars, aes(x = mpg, y = hp))+ geom_point()+ geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\nThe first thing to learn, is how to read the documentation for a function.\n?mean\nThe documentation has a few standard parts",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Errors and help</span>"
    ]
  },
  {
    "objectID": "code/Day1_04_ErrorsHelp.html#errors-and-getting-help",
    "href": "code/Day1_04_ErrorsHelp.html#errors-and-getting-help",
    "title": "\n4  Errors and help\n",
    "section": "",
    "text": "Function and package in curly brackets at top\nTitle\nDescription- Frequently not all that helpful, but sometimes useful\n\nUsage (Function and arguments)\n\n\n\nArguments - description of what all the arguments should contain\n\n\n\nValue - output\n\n\nReferences - articles or more info\nSee Also - similar functions or functions you might want if you want this one\n\n\nExamples - THE MOST USEFUL PART!\n\n\nSometimes there are other parts, including more details on statistical methods, but these are the basics.\n\n\n4.1.1 Exercise\nLook up the documentation for these functions and see if you can run the examples.\nrnorm\naov\nsample\nClick below to see the answer when you are done.\n\nCode?rnorm\n#rnorm generates a random number or numbers from a normal distribution. You can specify the mean and standard deviation\n\n#this generates 20 random numbers with a mean of 10 and a standard deviation of 3\nrnorm(20, mean = 10, sd =3)\n\n?aov\n#aov runs an ANOVA (analysis of variance), which tests for differences between groups\n\n#the examples use the \"npk\" dataset which is built into R\naov(yield ~ block + N * P + K, npk)\n\n?sample\n#this function takes a subsample of a larger set of number of a specified size \n\nx &lt;- 1:12\n# a random permutation\nsample(x)\n# bootstrap resampling -- only if length(x) &gt; 1 !\nsample(x, replace = TRUE)\n\n\n\n4.1.2 Online help options\n\nhttps://rseek.org/\nhttps://chatgpt.com/\nhttps://posit.co/resources/cheatsheets/",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Errors and help</span>"
    ]
  },
  {
    "objectID": "code/Day1_04_ErrorsHelp.html#closing-for-the-day",
    "href": "code/Day1_04_ErrorsHelp.html#closing-for-the-day",
    "title": "\n4  Errors and help\n",
    "section": "\n4.2 Closing for the day",
    "text": "4.2 Closing for the day\nHow to save your workspace image.\n\nsave.image()\n#also little save icon on environment\n\nHow to clear your workspace.\n\nrm(list = ls())\n#also little broom icon on the environment",
    "crumbs": [
      "Day 1 - An introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Errors and help</span>"
    ]
  },
  {
    "objectID": "code/Day2_01_Intro.html",
    "href": "code/Day2_01_Intro.html",
    "title": "Day 2 - Manipulating Data",
    "section": "",
    "text": "Our goal as scientists is to use our collected data to produce statistics and graphics that inform various management and scientific questions. The first step in this process is to properly format our data. Practically, this means learning how to manipulate data frames in R.\nAn outline for today’s lesson:\n\nImport Data\n\nLoad Packages\nImport Data\n\nSubset Data\n\nBasic Subsetting\nSubset by Column Names\nSubset by Row Values\nLogical Operators\nSubset by Column and Row\nSubset by Date\nSubset NA\n\nMutate Data\n\nAdd New Columns\nRelocate Columns\nModify Existing Columns\ncase_when\nRename\n\nSummarize\n\nSummarize Data\nGrouping Data\nPivot Data\n\nFinal Exercise",
    "crumbs": [
      "Day 2 - Manipulating Data"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html",
    "href": "code/Day2_02_Subset.html",
    "title": "\n5  Subset Data\n",
    "section": "",
    "text": "5.1 Basic Setup\nThe first step when working with data is almost always loading the relevant packages and importing it. One of the most important ones for this task is tidyverse, which we learned about yesterday. We’ll import it here:\nlibrary(tidyverse)\nlibrary(here)\nNext, we read in the data. As we learned yesterday, since I’m in a project, I can use relative file paths for import:\ndf_wq &lt;- read_csv('data/WQ_P8D7.csv')\nAs a reminder, we can use head to check what our data looks like:\nhead(df_wq)\n\n# A tibble: 6 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5             98        0.15\n2 D7      2020-01-22  0.67       0.87            82        0.21\n3 P8      2020-02-14  1.46       0.69            81        0.25\n4 D7      2020-02-20  2.15       0.5             86        0.14\n5 P8      2020-03-03  1.4        0.56            80        0.11\n6 D7      2020-03-06  1.89       1.13            93        0.22\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#data-structure",
    "href": "code/Day2_02_Subset.html#data-structure",
    "title": "\n5  Subset Data\n",
    "section": "\n5.2 Data Structure",
    "text": "5.2 Data Structure\nLet’s talk a bit about the structure of a data frame. Data frames are 2-dimensional objects (row x column).\n\n\n\n\n\n\n\n\nNote that, on the far-left side, there is a column of numbers separate from the data frame itself. This is called the row index. Similarly, every column has its own column index. Combined, this means every entry in a data frame has a unique, 2-dimensional index that’s defined by which row x column it’s in.\nIn R, the syntax for this is [row, column]. (Note that square brackets [ ] are used almost exclusively for indexing objects).\nSince we rarely want to work on the entire data frame at once, we can use these indices to subset our data.",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#basic-subsetting",
    "href": "code/Day2_02_Subset.html#basic-subsetting",
    "title": "\n5  Subset Data\n",
    "section": "\n5.3 Basic Subsetting",
    "text": "5.3 Basic Subsetting\nFor example, if I want to look at the value in the 1st row of the 2nd column, I can call the index [1,2]:\n\ndf_wq[1,2]\n\n# A tibble: 1 × 1\n  Date      \n  &lt;date&gt;    \n1 2020-01-16\n\n\nor, perhaps, the 2nd row of the 1st column:\n\ndf_wq[2,1]\n\n# A tibble: 1 × 1\n  Station\n  &lt;chr&gt;  \n1 D7     \n\n\nWe can also access an entire row or column at once:\n\ndf_wq[1,] # entire row\n\n# A tibble: 1 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64        0.5            98        0.15\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\ndf_wq[,1] # entire column\n\n# A tibble: 62 × 1\n   Station\n   &lt;chr&gt;  \n 1 P8     \n 2 D7     \n 3 P8     \n 4 D7     \n 5 P8     \n 6 D7     \n 7 P8     \n 8 D7     \n 9 P8     \n10 D7     \n# ℹ 52 more rows\n\n\nYou can also subset multiple columns/rows at once by using a : , which generates a sequence from the first value to the second value:\n\n# all rows, 2-4th columns\ndf_wq[,2:4] %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  Date        Chla Pheophytin\n  &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 2020-01-16  0.64       0.5 \n2 2020-01-22  0.67       0.87\n3 2020-02-14  1.46       0.69\n4 2020-02-20  2.15       0.5 \n5 2020-03-03  1.4        0.56\n6 2020-03-06  1.89       1.13\n\n\n\n5.3.1 Exercise\nTry to subset the 1st-2nd row and 3rd-4th column.\n\nCodedf_wq[1:2,3:4]\n\n\n\n\n\n\n\n\nOperators and Pipes\n\n\n\nYou’ll notice I used some new syntax, namely, the %&gt;%. This is called the pipe operator. Operators are functions that allows one to perform operations on other functions/variables. The : operator, for example, is a function that operates on a vector to generates a sequence. The pipe operator, specifically, is a function that allows you to chain together tidyverse commands. Using pipes helps keep code readable and easy to follow.",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#subset-by-column-names",
    "href": "code/Day2_02_Subset.html#subset-by-column-names",
    "title": "\n5  Subset Data\n",
    "section": "\n5.4 Subset by Column Names",
    "text": "5.4 Subset by Column Names\nTo ability to subset is very powerful. However, if we specify values by their numerical index, we can get confused about what we’re accessing. For example, if I wanted to work with DissAmmonia data, I would have to know that it’s the 4th column in my data frame. That can get unwieldy with complex datasets.\nInstead, we can use the column header to call a particular column:\n\n# index data from 'df_wq' in the 'DissAmmonia' column\ndf_wq['DissAmmonia'] %&gt;%\n  head()\n\n# A tibble: 6 × 1\n  DissAmmonia\n        &lt;dbl&gt;\n1        0.15\n2        0.21\n3        0.25\n4        0.14\n5        0.11\n6        0.22\n\n\nThis returns a tibble that only contains the relevant column.\n\nstr(df_wq['DissAmmonia'])\n\ntibble [62 × 1] (S3: tbl_df/tbl/data.frame)\n $ DissAmmonia: num [1:62] 0.15 0.21 0.25 0.14 0.11 0.22 0.05 0.05 0.05 0.05 ...\n\n\nWe can also call the column as a vector using the $ operator; this is the more common syntax.\n(Note: if your column name has spaces, surround the column name in back ticks ``)\n\n# call the DissAmmonia column\ndf_wq$DissAmmonia %&gt;%\n  head()\n\n[1] 0.15 0.21 0.25 0.14 0.11 0.22\n\n# surround in back ticks\ndf_wq$`DissAmmonia` %&gt;%\n  head()\n\n[1] 0.15 0.21 0.25 0.14 0.11 0.22\n\n\n\nstr(df_wq$DissAmmonia)\n\n num [1:62] 0.15 0.21 0.25 0.14 0.11 0.22 0.05 0.05 0.05 0.05 ...\n\n\nTo select multiple columns by name, we use the : operator within the select function from the dplyr package (in tidyverse):\n\ndf_wq %&gt;% select(Station:Pheophytin) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n  Station Date        Chla Pheophytin\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5 \n2 D7      2020-01-22  0.67       0.87\n3 P8      2020-02-14  1.46       0.69\n4 D7      2020-02-20  2.15       0.5 \n5 P8      2020-03-03  1.4        0.56\n6 D7      2020-03-06  1.89       1.13\n\n\nIf this is the only data I want to work with, I can store this as a unique object:\n\ndf_chlpheo &lt;- df_wq %&gt;% select(Station:Pheophytin)\n\n\nglimpse(df_chlpheo)\n\nRows: 62\nColumns: 4\n$ Station    &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\",…\n$ Date       &lt;date&gt; 2020-01-16, 2020-01-22, 2020-02-14, 2020-02-20, 2020-03-03…\n$ Chla       &lt;dbl&gt; 0.64, 0.67, 1.46, 2.15, 1.40, 1.89, 4.73, 1.74, 6.40, 2.79,…\n$ Pheophytin &lt;dbl&gt; 0.50, 0.87, 0.69, 0.50, 0.56, 1.13, 1.25, 0.89, 0.88, 0.85,…\n\n\nWe can also select specific columns by using a vector:\n\ndf_wq %&gt;% select(c(Station, Pheophytin)) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  Station Pheophytin\n  &lt;chr&gt;        &lt;dbl&gt;\n1 P8            0.5 \n2 D7            0.87\n3 P8            0.69\n4 D7            0.5 \n5 P8            0.56\n6 D7            1.13",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#subset-by-row-values",
    "href": "code/Day2_02_Subset.html#subset-by-row-values",
    "title": "\n5  Subset Data\n",
    "section": "\n5.5 Subset By Row Values",
    "text": "5.5 Subset By Row Values\nAnother common goal is to subset by particular row values – say, only a given station, date range, or analyte value range. Tidyverse also has functions for this! Specifically, we use filter from the dplyr package:\n\ndf_p8 &lt;- df_wq %&gt;% filter(Station == 'P8')\n\ndf_p8 %&gt;% head()\n\n# A tibble: 6 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5             98        0.15\n2 P8      2020-02-14  1.46       0.69            81        0.25\n3 P8      2020-03-03  1.4        0.56            80        0.11\n4 P8      2020-06-11  4.73       1.25            59        0.05\n5 P8      2020-07-13  6.4        0.88            63        0.05\n6 P8      2020-08-11 16.5        1.41            65        0.05\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\nTo check that this code worked correctly, we can look at all the unique values in the Station column using the unique function:\n\nunique(df_p8$Station)\n\n[1] \"P8\"",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#logical-operators",
    "href": "code/Day2_02_Subset.html#logical-operators",
    "title": "\n5  Subset Data\n",
    "section": "\n5.6 Logical Operators",
    "text": "5.6 Logical Operators\nNote we used another new symbol: ==. This is the equality operator, a type of logical operator.\nWe defined operators above as functions that perform operations on other functions/variables. It follows, then, that logical operators perform operators based on logical statements! Logical statements are the backbone of programming. Every single coding task (including every single function) can, at its core, be broken down into logical statements.\nTherefore, if you’re stuck on a coding task, try to reword it as a series of logical statements! That way, when you Google your questions (or put them into ChatGPT), you’ll get better and more precise answers.\nExample:“I want all of the data in df_wq where the Station is P8”\nbecomes\n“given df_wq , if the value in the Station column equals P8, return TRUE (keep the value)”\n\n\n\n\n\n\nBoolean Type\n\n\n\nWhen we use logical operators, how does the code know what values should be kept?\nLogical statements return a special type of output, called boolean. Boolean can only have one of two values: TRUE or FALSE. Other functions, like subsetting with [], can use this output to determine which values to keep.\n\n5 == 5\n\n[1] TRUE\n\n\n\n5 == 6\n\n[1] FALSE\n\n\n\n\nLet’s talk about some specific operators:\n\n5.6.1 equality (==) and negate equality (!=)\nEquality, as we saw above, tells the code to find all values from the right-hand side that are equal to the left-hand side.\n\ndf_wq %&gt;% filter(Station == 'P8') %&gt;%\n  head()\n\n# A tibble: 6 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5             98        0.15\n2 P8      2020-02-14  1.46       0.69            81        0.25\n3 P8      2020-03-03  1.4        0.56            80        0.11\n4 P8      2020-06-11  4.73       1.25            59        0.05\n5 P8      2020-07-13  6.4        0.88            63        0.05\n6 P8      2020-08-11 16.5        1.41            65        0.05\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\nNegate equality does the opposite; it gives us the values that do not match. It’s actually a combination of two operators: negate (!) and equality (==). ! is the general negate operator; it can be applied to any logical operator.\n\ndf_wq %&gt;% filter(Station != 'P8') %&gt;%\n  head()\n\n# A tibble: 6 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 D7      2020-01-22  0.67       0.87            82        0.21\n2 D7      2020-02-20  2.15       0.5             86        0.14\n3 D7      2020-03-06  1.89       1.13            93        0.22\n4 D7      2020-06-17  1.74       0.89            78        0.05\n5 D7      2020-07-16  2.79       0.85            80        0.05\n6 D7      2020-08-17  0.5        6.13            83        0.05\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\n5.6.2 and (&) and or (|)\nSometimes, we want to filter by multiple commands at once. We can use this using the logical operators and (&) or or (|):\n\ndf_wq %&gt;% filter(Station == 'P8' & Date == '2020-01-16')\n\n# A tibble: 1 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64        0.5            98        0.15\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\ndf_wq %&gt;% filter(Date == '2020-01-16' | Date == '2020-01-22')\n\n# A tibble: 2 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5             98        0.15\n2 D7      2020-01-22  0.67       0.87            82        0.21\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\n5.6.3 less than &lt; and greater than &gt;\n\nSometimes, we want all values above or below; we can use less/greater than (&lt; and &gt;) for this task. Similarly, we can use “less/greater than or equal to” (&lt;= and &gt;=)\n\ndf_wq %&gt;% filter(Date &gt;= '2020-02-01')\n\n# A tibble: 60 × 20\n   Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 P8      2020-02-14  1.46       0.69            81        0.25\n 2 D7      2020-02-20  2.15       0.5             86        0.14\n 3 P8      2020-03-03  1.4        0.56            80        0.11\n 4 D7      2020-03-06  1.89       1.13            93        0.22\n 5 P8      2020-06-11  4.73       1.25            59        0.05\n 6 D7      2020-06-17  1.74       0.89            78        0.05\n 7 P8      2020-07-13  6.4        0.88            63        0.05\n 8 D7      2020-07-16  2.79       0.85            80        0.05\n 9 P8      2020-08-11 16.5        1.41            65        0.05\n10 D7      2020-08-17  0.5        6.13            83        0.05\n# ℹ 50 more rows\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\ndf_wq %&gt;% filter(Date &lt;= '2020-06-30')\n\n# A tibble: 8 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5             98        0.15\n2 D7      2020-01-22  0.67       0.87            82        0.21\n3 P8      2020-02-14  1.46       0.69            81        0.25\n4 D7      2020-02-20  2.15       0.5             86        0.14\n5 P8      2020-03-03  1.4        0.56            80        0.11\n6 D7      2020-03-06  1.89       1.13            93        0.22\n7 P8      2020-06-11  4.73       1.25            59        0.05\n8 D7      2020-06-17  1.74       0.89            78        0.05\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\nAt times, it may make more sense to use the negate operator (ie. it’s easier to explain your reasoning that way). When using the negate operator with filter, we negate the entire statement use the syntax below:\n\ndf_wq %&gt;% filter(!(Date &gt;= '2020-06-30')) # negate greater than\n\n# A tibble: 8 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5             98        0.15\n2 D7      2020-01-22  0.67       0.87            82        0.21\n3 P8      2020-02-14  1.46       0.69            81        0.25\n4 D7      2020-02-20  2.15       0.5             86        0.14\n5 P8      2020-03-03  1.4        0.56            80        0.11\n6 D7      2020-03-06  1.89       1.13            93        0.22\n7 P8      2020-06-11  4.73       1.25            59        0.05\n8 D7      2020-06-17  1.74       0.89            78        0.05\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\nWhat if we want data in between two dates? We can use the & operator!\n\ndf_wq %&gt;% filter(Date &gt;= '2020-02-01' & Date &lt;= '2020-06-30')\n\n# A tibble: 6 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-02-14  1.46       0.69            81        0.25\n2 D7      2020-02-20  2.15       0.5             86        0.14\n3 P8      2020-03-03  1.4        0.56            80        0.11\n4 D7      2020-03-06  1.89       1.13            93        0.22\n5 P8      2020-06-11  4.73       1.25            59        0.05\n6 D7      2020-06-17  1.74       0.89            78        0.05\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\n5.6.4 in (%in%)\nWhat if we wanted to subset by five specific dates? We could string together multiple | commands, but that can become unwieldy to write.\nWhat if, instead, we had a vector of those five specific dates? Then we could subset by all the values in the data that match one of the values in that vector.\nThis is what the %in% function does:\n\ndf_wq %&gt;% filter(Date %in% c('2020-02-14','2020-03-06','2020-06-11','2021-03-05','2021-04-05'))\n\n# A tibble: 5 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-02-14  1.46       0.69            81       0.25 \n2 D7      2020-03-06  1.89       1.13            93       0.22 \n3 P8      2020-06-11  4.73       1.25            59       0.05 \n4 P8      2021-03-05  1.56       0.5            103       0.299\n5 P8      2021-04-05  2.62       1.1            116       0.063\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\n5.6.5 Exercise\nIn one filter function, how would I select data that’s either before 2020-02-28 or after 2022-11-01?\n\nCodedf_wq %&gt;% filter(Date &lt;= '2020-02-28' | Date &gt;= '2022-11-01')",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#subset-by-column-and-row",
    "href": "code/Day2_02_Subset.html#subset-by-column-and-row",
    "title": "\n5  Subset Data\n",
    "section": "\n5.7 Subset by Column and Row",
    "text": "5.7 Subset by Column and Row\nUsing our knowledge of pipes, it’s easy to subset by column and row at the same time!\n\ndf_wq %&gt;% filter(Date == '2020-01-16' | Date == '2020-01-22') %&gt;% select(Station:Pheophytin)\n\n# A tibble: 2 × 4\n  Station Date        Chla Pheophytin\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5 \n2 D7      2020-01-22  0.67       0.87\n\n\n\n\n\n\n\n\nFormatting Code\n\n\n\nNotice that above all the code is on the same line. This can be difficult to read. You can get around this by formatting your code. Personally, I like having different functions on different lines:\n\ndf_wq %&gt;%\n  filter(Date == '2020-01-16' | Date == '2020-01-22') %&gt;%\n  select(Station:Pheophytin)\n\nYou can also use ctrl+shift+A to auto-format code! Note that it looks different from above; this is fine. As long as you deem the code readable (and it works), you’re set.\n\n# original\ndf_wq %&gt;% filter(Date == '2020-01-16' | Date == '2020-01-22') %&gt;% select(Station:Pheophytin)\n\n# ctrl+shift+A\ndf_wq %&gt;% filter(Date == '2020-01-16' |\n                   Date == '2020-01-22') %&gt;% select(Station:Pheophytin)",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#subset-na",
    "href": "code/Day2_02_Subset.html#subset-na",
    "title": "\n5  Subset Data\n",
    "section": "\n5.8 Subset NA\n",
    "text": "5.8 Subset NA\n\nThe final type of subset that we’ll cover is by NA data.\nNA is a logical constant of length 1 which contains a missing value indicator:\n\ntypeof(NA) # NA\n\n[1] \"logical\"\n\ntypeof('NA') # not the same\n\n[1] \"character\"\n\n\nSometimes, we want to select only NA data or omit it entirely. Looking at the DON column, we see that there are NAs:\n\nunique(df_wq$DON) # returns unique values\n\n [1]   NA 0.30 0.20 0.10 0.50 0.22 0.19 0.11 0.53 0.13 0.46 0.25 0.37 0.29 0.17\n[16] 0.24 0.27 0.21 0.35 0.12 0.18 0.16 1.07 0.33 0.40 0.44 0.43 0.42 0.38\n\n\nA quicker way to check this is the is.na function:\n\nany(is.na(df_wq$DON))\n\n[1] TRUE\n\n\nThis returns a logical vector. If I want to subset by this, I can use the filter function:\n\ndf_wq %&gt;%\n  filter(is.na(df_wq$DON)) %&gt;%\n  select(Station, Date, DON)\n\n# A tibble: 6 × 3\n  Station Date         DON\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;\n1 P8      2020-01-16    NA\n2 D7      2020-01-22    NA\n3 P8      2020-02-14    NA\n4 D7      2020-02-20    NA\n5 P8      2020-03-03    NA\n6 D7      2020-03-06    NA\n\n\nQuestion: What operator would I use if I want to filter all data except NAs?\n\n5.8.1 Exercise\nHow would I write this statement? (Hint: examples are above)\n\nCodedf_wq %&gt;%\n  filter(!is.na(df_wq$DON)) %&gt;% # use the ! operator before the function to negate\n  select(Station, Date, DON)",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_02_Subset.html#subset-by-dates",
    "href": "code/Day2_02_Subset.html#subset-by-dates",
    "title": "\n5  Subset Data\n",
    "section": "\n5.9 Subset by Dates",
    "text": "5.9 Subset by Dates\n\n\n\n\n\n\nImporting Dates Correctly\n\n\n\nDates can be very complicated in R. One common issue is that date formats can display differently from the same file! Here’s an example:\n\ndf_dash_dates &lt;- read_csv('data/WQ_P8D7.csv', show_col_types = FALSE) %&gt;%\n  select(Date) %&gt;%\n  head()\n\ndf_slash_dates &lt;- read_csv('data/WQ_P8D7_moddates.csv', show_col_types = FALSE) %&gt;%\n  select(Date) %&gt;%\n  head() \n\ndf_dash_dates\n\n# A tibble: 6 × 1\n  Date      \n  &lt;date&gt;    \n1 2020-01-16\n2 2020-01-22\n3 2020-02-14\n4 2020-02-20\n5 2020-03-03\n6 2020-03-06\n\ndf_slash_dates\n\n# A tibble: 6 × 1\n  Date     \n  &lt;chr&gt;    \n1 1/16/2020\n2 1/22/2020\n3 2/14/2020\n4 2/20/2020\n5 3/3/2020 \n6 3/6/2020 \n\n\nWhat happened? In this case, it’s an issue of opening the file in Excel and saving it. When you do this, Excel can change the date format without you even knowing. One workaround for this issue to use the parse_date_time function; we will go into this in more detail on Day 4.\nThe format Year-Month-Day is the “correct” Date format in R (with a 4 digit year). So how do we convert? There are many ways, but one is using the as.Date function in R, which can be nested in the mutate function.\nTo do this, we need to specify the format the other date is in.This is done using something called “format codes” that represent parts of the date and time. These codes must match the structure of your data, including any separators like slashes, dashes, or spaces.\nA cheat sheet for format codes:\n\n\n%Y: year (4 digit) – 2005\n\n%y: year (2 digit) – 05\n\n%m: month (numeric) – 01\n\n%B: month (full name) – January\n\n%b: month (abbreviation) – Jan\n\n%d: day\n\n%H: hour\n\n%M: minute\n\n%S: second\n\nAn example:\n\ntest_date &lt;- '04/05/2021'\n\nnew_date &lt;- as.Date(test_date, format = '%m/%d/%Y')\n\nnew_date\n\n[1] \"2021-04-05\"\n\nclass(new_date)\n\n[1] \"Date\"\n\n\n\n5.9.1 Exercise\nConvert April 5 2021 to a date character\n\ntest_date2 &lt;- 'April 5 2021'\n\nas.Date(test_date2, format = '%B %d %Y')\n\n[1] \"2021-04-05\"\n\n\n\n\n\nWhat if I want to subset all values in a given year? If I had a vector of all the years in my dataset, then I could simply use %in% (or, if there’s only one year, ==)! But how would I get that vector?\nThe lubridate package allows you to manipulate dates. Since dates are complicated in R, we won’t go into too much detail. However, a few useful functions are day, month, and year, which return the days, months, and years (as vectors) from a vector of dates that are of the Date class:\nOriginal:\n\nlibrary(lubridate)\n\ndf_wq$Date %&gt;%\n  head()\n\n[1] \"2020-01-16\" \"2020-01-22\" \"2020-02-14\" \"2020-02-20\" \"2020-03-03\"\n[6] \"2020-03-06\"\n\n\n\nclass(df_wq$Date) # check that it's the correct class\n\n[1] \"Date\"\n\n\nDays:\n\nday(df_wq$Date)\n\n [1] 16 22 14 20  3  6 11 17 13 16 11 17  9 24  8 13  6 12  5 10  5  8  5 10  3\n[26]  8 16 21 16 19 10 16 13 18 10 16 10 15 12 14 16 27 27 27 25 22 20 19 18 19\n[51]  7  9 11 22 23 22 20 17 15 14 15 14\n\n\nMonths:\n\nmonth(df_wq$Date)\n\n [1]  1  1  2  2  3  3  6  6  7  7  8  8  9  9 10 10 11 11  3  3  4  4  5  5  6\n[26]  6  7  7  8  8  9  9 10 10 11 11 12 12  1  2  3  4  5  6  7  8  9 10 11 12\n[51]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nYears:\n\nyear(df_wq$Date)\n\n [1] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020\n[16] 2020 2020 2020 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021\n[31] 2021 2021 2021 2021 2021 2021 2021 2021 2022 2022 2022 2022 2022 2022 2022\n[46] 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022\n[61] 2022 2022\n\n\nOne use for these functions is to subset. Say we want all entries from the year 2021:\n\ndf_wq %&gt;% filter(year(Date) == '2021') %&gt;%\n  head()\n\n# A tibble: 6 × 20\n  Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2021-03-05  1.56       0.5          103         0.299\n2 D7      2021-03-10  4.77       0.5           97.9       0.135\n3 P8      2021-04-05  2.62       1.1          116         0.063\n4 D7      2021-04-08  3.28       0.83          93.9       0.078\n5 P8      2021-05-05  4.73       1.48          89.9       0.05 \n6 D7      2021-05-10  1.85       0.55         100         0.093\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\n# same thing but using negate (negate the years 2020 and 2022)\ndf_wq %&gt;% filter(\n  !(year(Date) %in% c('2020','2022'))\n  )\n\n# A tibble: 20 × 20\n   Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 P8      2021-03-05  1.56       0.5          103         0.299\n 2 D7      2021-03-10  4.77       0.5           97.9       0.135\n 3 P8      2021-04-05  2.62       1.1          116         0.063\n 4 D7      2021-04-08  3.28       0.83          93.9       0.078\n 5 P8      2021-05-05  4.73       1.48          89.9       0.05 \n 6 D7      2021-05-10  1.85       0.55         100         0.093\n 7 P8      2021-06-03  4.39       0.5           78.1       0.056\n 8 D7      2021-06-08  4.21       1.2           96.8       0.058\n 9 P8      2021-07-16  4.3        2.28          49.2       0.05 \n10 D7      2021-07-21  6.54       1.15          93.4       0.05 \n11 P8      2021-08-16  5.56       1.2           46.4       0.05 \n12 D7      2021-08-19  6.76       4.03          91.1       0.05 \n13 P8      2021-09-10  3.85       1.22          57.5       0.069\n14 D7      2021-09-16  2.74       1.32          91.5       0.05 \n15 P8      2021-10-13  1.97       0.57          74.9       0.069\n16 D7      2021-10-18  2.95       2.86          94.9       0.073\n17 P8      2021-11-10  1.25       0.92          59.8       0.118\n18 D7      2021-11-16  1.52       1.38          85.6       0.186\n19 P8      2021-12-10  1.52       0.7           79.4       0.09 \n20 D7      2021-12-15  1.17       1.55          91.5       0.192\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\n5.9.2 Exercise\nSubset to only include data from April 2021 (hint: use the & operator)\n\nCodedf_wq %&gt;% filter(year(Date) == '2021' & month(Date) == '4') %&gt;%\n  head()",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Subset Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_03_Mutate.html",
    "href": "code/Day2_03_Mutate.html",
    "title": "\n6  Mutate Data\n",
    "section": "",
    "text": "6.1 Introduction\nSometimes, we want to modify existing columns values or even add new ones. This is where the mutate function from the dplyr package comes in.",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mutate Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_03_Mutate.html#add-new-columns",
    "href": "code/Day2_03_Mutate.html#add-new-columns",
    "title": "\n6  Mutate Data\n",
    "section": "\n6.2 Add New Columns",
    "text": "6.2 Add New Columns\nWith mutate, it’s simple to add new columns. First, let’s look at our current ones:\n\nglimpse(df_wq)\n\nRows: 62\nColumns: 20\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2020-01-16, 2020-01-22, 2020-02-14, 2020-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.64, 0.67, 1.46, 2.15, 1.40, 1.89, 4.73, 1.74, 6.4…\n$ Pheophytin         &lt;dbl&gt; 0.50, 0.87, 0.69, 0.50, 0.56, 1.13, 1.25, 0.89, 0.8…\n$ TotAlkalinity      &lt;dbl&gt; 98.0, 82.0, 81.0, 86.0, 80.0, 93.0, 59.0, 78.0, 63.…\n$ DissAmmonia        &lt;dbl&gt; 0.150, 0.210, 0.250, 0.140, 0.110, 0.220, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 2.800, 0.490, 1.700, 0.480, 1.600, 0.380, 1.070, 0.…\n$ DOC                &lt;dbl&gt; 3.90, 0.27, 2.80, 0.39, 2.00, 0.19, 2.80, 1.20, 3.1…\n$ TOC                &lt;dbl&gt; 4.10, 0.32, 2.50, 0.41, 2.10, 0.20, 2.80, 1.20, 3.1…\n$ DON                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0.30, 0.20, 0.30, 0.10, 0.5…\n$ TotPhos            &lt;dbl&gt; 0.310, 0.082, 0.130, 0.130, 0.190, 0.100, 0.188, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.200, 0.071, 0.130, 0.065, 0.140, 0.082, 0.177, 0.…\n$ TDS                &lt;dbl&gt; 380, 9500, 340, 5800, 290, 8700, 280, 7760, 227, 11…\n$ TSS                &lt;dbl&gt; 8.9, 38.0, 2.2, 18.0, 1.4, 28.0, 6.6, 35.6, 5.3, 23…\n$ TKN                &lt;dbl&gt; 0.520, 0.480, 0.430, 0.250, 0.400, 0.200, 0.400, 0.…\n$ Depth              &lt;dbl&gt; 28.9, 18.8, 39.0, 7.1, 39.0, 7.2, 37.1, 5.2, 36.7, …\n$ Secchi             &lt;dbl&gt; 116, 30, 212, 52, 340, 48, 100, 40, 160, 44, 120, 6…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 3, 2, 3, 2, 4, 2, 3, 2, 2, 1, 1, …\n$ SpCndSurface       &lt;dbl&gt; 667, 15532, 647, 11369, 530, 16257, 503, 12946, 404…\n$ WTSurface          &lt;dbl&gt; 9.67, 9.97, 11.09, 12.51, 13.97, 13.81, 23.46, 21.1…\n\n\n\nNow let’s add a new column, Lab, where every value is “BSA”:\n\ndf_wq %&gt;%\n  mutate(Lab = 'BSA') %&gt;%\n  glimpse()\n\nRows: 62\nColumns: 21\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2020-01-16, 2020-01-22, 2020-02-14, 2020-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.64, 0.67, 1.46, 2.15, 1.40, 1.89, 4.73, 1.74, 6.4…\n$ Pheophytin         &lt;dbl&gt; 0.50, 0.87, 0.69, 0.50, 0.56, 1.13, 1.25, 0.89, 0.8…\n$ TotAlkalinity      &lt;dbl&gt; 98.0, 82.0, 81.0, 86.0, 80.0, 93.0, 59.0, 78.0, 63.…\n$ DissAmmonia        &lt;dbl&gt; 0.150, 0.210, 0.250, 0.140, 0.110, 0.220, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 2.800, 0.490, 1.700, 0.480, 1.600, 0.380, 1.070, 0.…\n$ DOC                &lt;dbl&gt; 3.90, 0.27, 2.80, 0.39, 2.00, 0.19, 2.80, 1.20, 3.1…\n$ TOC                &lt;dbl&gt; 4.10, 0.32, 2.50, 0.41, 2.10, 0.20, 2.80, 1.20, 3.1…\n$ DON                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0.30, 0.20, 0.30, 0.10, 0.5…\n$ TotPhos            &lt;dbl&gt; 0.310, 0.082, 0.130, 0.130, 0.190, 0.100, 0.188, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.200, 0.071, 0.130, 0.065, 0.140, 0.082, 0.177, 0.…\n$ TDS                &lt;dbl&gt; 380, 9500, 340, 5800, 290, 8700, 280, 7760, 227, 11…\n$ TSS                &lt;dbl&gt; 8.9, 38.0, 2.2, 18.0, 1.4, 28.0, 6.6, 35.6, 5.3, 23…\n$ TKN                &lt;dbl&gt; 0.520, 0.480, 0.430, 0.250, 0.400, 0.200, 0.400, 0.…\n$ Depth              &lt;dbl&gt; 28.9, 18.8, 39.0, 7.1, 39.0, 7.2, 37.1, 5.2, 36.7, …\n$ Secchi             &lt;dbl&gt; 116, 30, 212, 52, 340, 48, 100, 40, 160, 44, 120, 6…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 3, 2, 3, 2, 4, 2, 3, 2, 2, 1, 1, …\n$ SpCndSurface       &lt;dbl&gt; 667, 15532, 647, 11369, 530, 16257, 503, 12946, 404…\n$ WTSurface          &lt;dbl&gt; 9.67, 9.97, 11.09, 12.51, 13.97, 13.81, 23.46, 21.1…\n$ Lab                &lt;chr&gt; \"BSA\", \"BSA\", \"BSA\", \"BSA\", \"BSA\", \"BSA\", \"BSA\", \"B…\n\n\nWe can even add columns that are combinations of other ones:\nadd two numeric columns together\n\ndf_wq %&gt;%\n  mutate(ChlPheo = Chla + Pheophytin) %&gt;%\n  select(Station, Date, Chla, Pheophytin, ChlPheo) %&gt;%\n  head()\n\n# A tibble: 6 × 5\n  Station Date        Chla Pheophytin ChlPheo\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 P8      2020-01-16  0.64       0.5     1.14\n2 D7      2020-01-22  0.67       0.87    1.54\n3 P8      2020-02-14  1.46       0.69    2.15\n4 D7      2020-02-20  2.15       0.5     2.65\n5 P8      2020-03-03  1.4        0.56    1.96\n6 D7      2020-03-06  1.89       1.13    3.02\n\n\nconcatenate two character columns\n\ndf_wq %&gt;%\n  mutate(StatDate = paste(Station, Date, sep = '___')) %&gt;%\n  select(Station, Date, StatDate) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  Station Date       StatDate       \n  &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt;          \n1 P8      2020-01-16 P8___2020-01-16\n2 D7      2020-01-22 D7___2020-01-22\n3 P8      2020-02-14 P8___2020-02-14\n4 D7      2020-02-20 D7___2020-02-20\n5 P8      2020-03-03 P8___2020-03-03\n6 D7      2020-03-06 D7___2020-03-06\n\n\n\n\n\n\n\n\nRelocate Columns\n\n\n\nSometimes, we want to re-arrange our column order. For example, with our new Lab column, we might want it in the beginning with the other metadata. Here, we use the relocate function:\n\ndf_wq %&gt;%\n  mutate(Lab = 'BSA') %&gt;%\n  relocate(Lab, .after = Date) %&gt;%\n  head()\n\n# A tibble: 6 × 21\n  Station Date       Lab    Chla Pheophytin TotAlkalinity DissAmmonia\n  &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 P8      2020-01-16 BSA    0.64       0.5             98        0.15\n2 D7      2020-01-22 BSA    0.67       0.87            82        0.21\n3 P8      2020-02-14 BSA    1.46       0.69            81        0.25\n4 D7      2020-02-20 BSA    2.15       0.5             86        0.14\n5 P8      2020-03-03 BSA    1.4        0.56            80        0.11\n6 D7      2020-03-06 BSA    1.89       1.13            93        0.22\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mutate Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_03_Mutate.html#modify-existing-columns",
    "href": "code/Day2_03_Mutate.html#modify-existing-columns",
    "title": "\n6  Mutate Data\n",
    "section": "\n6.3 Modify Existing Columns",
    "text": "6.3 Modify Existing Columns\nWe can also use mutate to modify existing columns. For example, say we want to add 20 to every Chla value:\n\ndf_wq %&gt;%\n  mutate(Chla = Chla+20)\n\n# A tibble: 62 × 20\n   Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 P8      2020-01-16  20.6       0.5             98        0.15\n 2 D7      2020-01-22  20.7       0.87            82        0.21\n 3 P8      2020-02-14  21.5       0.69            81        0.25\n 4 D7      2020-02-20  22.2       0.5             86        0.14\n 5 P8      2020-03-03  21.4       0.56            80        0.11\n 6 D7      2020-03-06  21.9       1.13            93        0.22\n 7 P8      2020-06-11  24.7       1.25            59        0.05\n 8 D7      2020-06-17  21.7       0.89            78        0.05\n 9 P8      2020-07-13  26.4       0.88            63        0.05\n10 D7      2020-07-16  22.8       0.85            80        0.05\n# ℹ 52 more rows\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\nWe can even change data types! This is useful if, say, you have character column that should be numeric, or a character column that should be factor. We do this by using the as.numeric and as.factor functions (as.character is also an option):\n\ndf_wq %&gt;%\n  mutate(Chla = as.numeric(Chla),\n         Station = as.factor(Station)) %&gt;%\n  select(Station, Chla) %&gt;%\n  str()\n\ntibble [62 × 2] (S3: tbl_df/tbl/data.frame)\n $ Station: Factor w/ 2 levels \"D7\",\"P8\": 2 1 2 1 2 1 2 1 2 1 ...\n $ Chla   : num [1:62] 0.64 0.67 1.46 2.15 1.4 1.89 4.73 1.74 6.4 2.79 ...",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mutate Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_03_Mutate.html#case_when",
    "href": "code/Day2_03_Mutate.html#case_when",
    "title": "\n6  Mutate Data\n",
    "section": "\n6.4 case_when\n",
    "text": "6.4 case_when\n\nSometimes you only want to change certain values in a column. For example, maybe you want to add 20 to chla values, but only for rows where the station is P8. You can do this using the case_when() function for this.\ncase_when() lets you assign new values based on conditions. It takes each row and sequentially compared it to each condition, using the value for the first one that is true. The format looks like this:\ncase_when(\n  condition1 ~ value1,\n  condition2 ~ value2,\n  TRUE ~ default_value\n  )\nSince every row must get a value, it’s important to end with TRUE ~ result to handle all remaining cases.\nTo go back to our example, the code would look like this:\n\ndf_wq %&gt;%\n  mutate(\n    Chla =\n      case_when(Station == 'P8' ~ Chla + 20, # if station is P8, add 20\n              TRUE ~ Chla) # else, keep as previous value\n  ) %&gt;%\n  select(c(Station:Date), Chla)\n\n# A tibble: 62 × 3\n   Station Date        Chla\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;\n 1 P8      2020-01-16 20.6 \n 2 D7      2020-01-22  0.67\n 3 P8      2020-02-14 21.5 \n 4 D7      2020-02-20  2.15\n 5 P8      2020-03-03 21.4 \n 6 D7      2020-03-06  1.89\n 7 P8      2020-06-11 24.7 \n 8 D7      2020-06-17  1.74\n 9 P8      2020-07-13 26.4 \n10 D7      2020-07-16  2.79\n# ℹ 52 more rows\n\n\nNote that case_when is nested in the mutate function and that we had to specify which column we wanted to manipulate (in this case, Chla). As you may assume, since it’s part of mutate, this means we can also create new columns via this process.\n\n6.4.0.1 Exercise\nNow its your turn to try out these summarizing functions we just learned about.\n\nUse case_when to multiple Pheophytin values by 2 for station D7.\nUse case_when (nested in mutate) to create a new column, DON_ND, where the value is 0 when when DON is NA (ie. a non-detect)HINT: we can use is.na to check if a column is NA\n\nClick below for the answer when you are done!\n\nCodedf_wq %&gt;%\n  mutate(\n    Test = \n      case_when(is.na(DON) ~ 0,\n                TRUE ~ DON)\n  )",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mutate Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_03_Mutate.html#rename",
    "href": "code/Day2_03_Mutate.html#rename",
    "title": "\n6  Mutate Data\n",
    "section": "\n6.5 Rename",
    "text": "6.5 Rename\nSometimes, we want to rename our columns (eg. if working with multiple data frames, we want the columns to be standardized). We can do this using the rename function from the dplyr package:\n\ndf_wq %&gt;%\n  rename(StationCode = Station)\n\n# A tibble: 62 × 20\n   StationCode Date        Chla Pheophytin TotAlkalinity DissAmmonia\n   &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 P8          2020-01-16  0.64       0.5             98        0.15\n 2 D7          2020-01-22  0.67       0.87            82        0.21\n 3 P8          2020-02-14  1.46       0.69            81        0.25\n 4 D7          2020-02-20  2.15       0.5             86        0.14\n 5 P8          2020-03-03  1.4        0.56            80        0.11\n 6 D7          2020-03-06  1.89       1.13            93        0.22\n 7 P8          2020-06-11  4.73       1.25            59        0.05\n 8 D7          2020-06-17  1.74       0.89            78        0.05\n 9 P8          2020-07-13  6.4        0.88            63        0.05\n10 D7          2020-07-16  2.79       0.85            80        0.05\n# ℹ 52 more rows\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mutate Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_04_Summarize.html",
    "href": "code/Day2_04_Summarize.html",
    "title": "\n7  Summarizing Data\n",
    "section": "",
    "text": "7.1 Basic Summarizing\nA common task scientists need to do is summarizing or aggregating a data set. We’ll start with some basic R functions for this task and then move into some more powerful R functions from the dplyr package to group and summarize your data.\nFor a basic overall summary of your data set, use the summary function. Here is what that looks like when you use it with the EMP water quality data set:\n# Simple overall summary statistics of entire data frame\nsummary(df_wq)\n\n   Station               Date                 Chla          Pheophytin   \n Length:62          Min.   :2020-01-16   Min.   : 0.500   Min.   :0.500  \n Class :character   1st Qu.:2020-10-19   1st Qu.: 1.530   1st Qu.:0.830  \n Mode  :character   Median :2021-09-13   Median : 2.515   Median :1.115  \n                    Mean   :2021-08-12   Mean   : 3.042   Mean   :1.350  \n                    3rd Qu.:2022-05-16   3rd Qu.: 4.188   3rd Qu.:1.472  \n                    Max.   :2022-12-19   Max.   :16.510   Max.   :6.130  \n                                                                         \n TotAlkalinity     DissAmmonia      DissNitrateNitrite      DOC       \n Min.   : 46.40   Min.   :0.05000   Min.   :0.1600     Min.   :0.190  \n 1st Qu.: 77.03   1st Qu.:0.05000   1st Qu.:0.3337     1st Qu.:1.600  \n Median : 84.60   Median :0.06850   Median :0.6830     Median :2.400  \n Mean   : 82.28   Mean   :0.09485   Mean   :1.2140     Mean   :2.751  \n 3rd Qu.: 90.95   3rd Qu.:0.11675   3rd Qu.:1.7150     3rd Qu.:3.700  \n Max.   :116.00   Max.   :0.29900   Max.   :5.4700     Max.   :9.500  \n                                                                      \n      TOC             DON            TotPhos       DissOrthophos   \n Min.   :0.200   Min.   :0.1000   Min.   :0.0820   Min.   :0.0650  \n 1st Qu.:1.500   1st Qu.:0.1900   1st Qu.:0.1190   1st Qu.:0.0930  \n Median :2.350   Median :0.2500   Median :0.1525   Median :0.1100  \n Mean   :2.734   Mean   :0.2827   Mean   :0.2052   Mean   :0.1837  \n 3rd Qu.:3.675   3rd Qu.:0.3500   3rd Qu.:0.2893   3rd Qu.:0.2838  \n Max.   :9.100   Max.   :1.0700   Max.   :0.4900   Max.   :0.4740  \n                 NA's   :6                                         \n      TDS               TSS               TKN             Depth      \n Min.   :  152.0   Min.   :  1.400   Min.   :0.1490   Min.   : 5.20  \n 1st Qu.:  307.5   1st Qu.:  4.575   1st Qu.:0.3020   1st Qu.: 6.20  \n Median : 2169.0   Median : 13.100   Median :0.3950   Median :12.80  \n Mean   : 5819.7   Mean   : 26.906   Mean   :0.4295   Mean   :21.18  \n 3rd Qu.:12125.0   3rd Qu.: 39.500   3rd Qu.:0.5222   3rd Qu.:37.33  \n Max.   :15800.0   Max.   :105.000   Max.   :1.4400   Max.   :42.00  \n                                                                     \n     Secchi        Microcystis     SpCndSurface       WTSurface    \n Min.   : 20.00   Min.   :1.000   Min.   :  278.0   Min.   : 9.06  \n 1st Qu.: 40.00   1st Qu.:1.000   1st Qu.:  548.8   1st Qu.:13.51  \n Median : 68.00   Median :1.000   Median : 3714.0   Median :19.05  \n Mean   : 93.49   Mean   :1.548   Mean   : 9771.1   Mean   :18.18  \n 3rd Qu.:144.00   3rd Qu.:2.000   3rd Qu.:20329.0   3rd Qu.:22.11  \n Max.   :340.00   Max.   :4.000   Max.   :25278.0   Max.   :27.01  \n NA's   :1\nYou can see that R provided a set of simple summary statistics (min, 25th and 75th quartiles, median, mean, max) for each column in the data frame. If you are interested in summary statistics for a single column in the data frame, you can use the data$column notation to subset your data set. If we wanted summary statistics of just chlorophyll-a, you could use:\n# Summary of one column\nsummary(df_wq$Chla)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.500   1.530   2.515   3.042   4.188  16.510\nR provided the same set of simple summary statistics for just the Chla column.\nNow, we’ll introduce a more powerful summarizing function from the dplyr package called summarize. We’ll start by using it to calculate some simple summary statistics. First, we’ll calculate the overall average of all chlorophyll-a data in the data set.\n# Calculate overall mean of one column\ndf_wq %&gt;%\n  summarize(mean_Chla = mean(Chla))\n\n# A tibble: 1 × 1\n  mean_Chla\n      &lt;dbl&gt;\n1      3.04\nYou may have noticed that, unlike the summary function, summarize provides the summarized data in a tibble or data frame. This is useful if you intend to continue working with the data. Also note that I provided a name for the summarized data column “mean_Chla”, which is a lot like the dplyr::mutate function we learned about earlier.\nYou can calculate multiple values at once by providing additional arguments to summarize. For example, let’s calculate the overall averages of both chlorophyll-a and Pheophytin.\n# Calculate overall mean of multiple columns\ndf_wq %&gt;% summarize(mean_Chla = mean(Chla), mean_Pheo = mean(Pheophytin))\n\n# A tibble: 1 × 2\n  mean_Chla mean_Pheo\n      &lt;dbl&gt;     &lt;dbl&gt;\n1      3.04      1.35\nR provides a 2-column tibble with our desired summary statistics. When using summarize, be mindful of NA values in your data columns. For example, DON has a few NA values - here is what happens when we try to calculate its mean:\ndf_wq %&gt;% summarize(mean_DON = mean(DON))\n\n# A tibble: 1 × 1\n  mean_DON\n     &lt;dbl&gt;\n1       NA\nNotice that it returns NA. If you want summarize to ignore or drop the NA values when making calculations, you’ll need to add the na.rm = TRUE argument to your summary function which in this case is mean.\ndf_wq %&gt;% summarize(mean_DON = mean(DON, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_DON\n     &lt;dbl&gt;\n1    0.283\nNow, R returns the overall average DON value after ignoring the NA values.",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_04_Summarize.html#basic-summarizing",
    "href": "code/Day2_04_Summarize.html#basic-summarizing",
    "title": "\n7  Summarizing Data\n",
    "section": "",
    "text": "7.1.0.1 Exercise\nNow its your turn to try out these summarizing functions we just learned about.\n\nUse summary to provide simple summary statistics for “Secchi” and “WTSurface”.HINT: Run the function on one column at a time.\nNow, use summarize to calculate the overall minimum value for “Secchi”.HINT: Watch out for NA values!\nAdd the minimum value for “WTSurface” to the summarize function used above, assign it to an object, and print it to view the results.\n\nClick below for the answer when you are done!\n\nCode# Use summary to calculate simple summary statistics for Secchi\nsummary(df_wq$Secchi)\n\n# Use summary to calculate simple summary statistics for WTSurface\nsummary(df_wq$WTSurface)\n\n# Use summarize to calculate the overall minimum value for Secchi\ndf_wq %&gt;% summarize(min_Secchi = min(Secchi, na.rm = TRUE))\n\n# Add the minimum value for \"WTSurface\" and assign it to an object called \"df_wq_min\"\ndf_wq_min &lt;- df_wq %&gt;% \n  summarize(\n    min_Secchi = min(Secchi, na.rm = TRUE),\n    min_WTSurface = min(WTSurface)\n  )\n\n# Print df_wq_min to see results\ndf_wq_min",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_04_Summarize.html#grouping",
    "href": "code/Day2_04_Summarize.html#grouping",
    "title": "\n7  Summarizing Data\n",
    "section": "\n7.2 Grouping",
    "text": "7.2 Grouping\nCalculating overall summary statistics is useful, but the real power of summarize becomes more apparent when its used in combination with the group_by function (also within the dplyr package). Using group_by with summarize allows for calculating summary statistics for groups of data within your data set. Examples include averages for each station, seasonal and annual statistics, or other combinations that you can imagine. Here is a simple example using these two functions to calculate overall average chlorophyll-a values for each station in the EMP water quality data set (D7 and P8).\n\n# Group by Station\ndf_wq %&gt;% group_by(Station) %&gt;% summarize(mean_Chla = mean(Chla))\n\n# A tibble: 2 × 2\n  Station mean_Chla\n  &lt;chr&gt;       &lt;dbl&gt;\n1 D7           2.71\n2 P8           3.37\n\n\nYou’ll see that now we have an additional column added to the tibble for “Station”, and the “mean_Chla” column contains the average values for each station. As with the example in the Basic Summarizing section above, you can calculate multiple values at once for each group by providing additional arguments to summarize.\n\n# Calculate more than one summary statistic within `summarize`\ndf_wq %&gt;% \n  group_by(Station) %&gt;% \n  summarize(\n    min_Chla = min(Chla),\n    mean_Chla = mean(Chla),\n    median_Chla = median(Chla),\n    max_Chla = max(Chla),\n    sd_Chla = sd(Chla)\n  )\n\n# A tibble: 2 × 6\n  Station min_Chla mean_Chla median_Chla max_Chla sd_Chla\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 D7          0.5       2.71        2.21     7.36    1.81\n2 P8          0.57      3.37        2.67    16.5     2.92\n\n\nYou can also group by more than one variable at a time. For example, the EMP water quality data set contains data from multiple years (2020-2022). We can calculate the same series of summary statistics for chlorophyll-a in the prior example for each station and year combination.\n\n# First, we add a second grouping variable for year, creating a new object for\n  # the resulting data frame\ndf_wq_c &lt;- df_wq %&gt;% mutate(Year = year(Date))\n\n# Next, we calculate summary statistics for Chla for each station and year\n  # combination\ndf_wq_c %&gt;% \n  group_by(Station, Year) %&gt;% \n  summarize(\n    min_Chla = min(Chla),\n    mean_Chla = mean(Chla),\n    median_Chla = median(Chla),\n    max_Chla = max(Chla),\n    sd_Chla = sd(Chla)\n  )\n\n# A tibble: 6 × 7\n# Groups:   Station [2]\n  Station  Year min_Chla mean_Chla median_Chla max_Chla sd_Chla\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 D7       2020     0.5       1.53        1.74     2.79   0.749\n2 D7       2021     1.17      3.58        3.12     6.76   1.97 \n3 D7       2022     0.67      2.88        2.51     7.36   1.87 \n4 P8       2020     0.64      4.48        2.81    16.5    4.88 \n5 P8       2021     1.25      3.18        3.24     5.56   1.57 \n6 P8       2022     0.57      2.70        2.58     5.24   1.54 \n\n\nWow, now we are starting to get somewhere with summarizing our data set. You’ll see that now we have another column added to the tibble for “Year” in addition to “Station” with the desired summary statistics for each combination in the following columns. You may also have noticed that the printout of the tibble indicates that it is still grouped by “Station”. This is because the default behavior of using summarize after group_by is to drop the last level of grouping (“Year”) in its output. It is always good practice to ungroup a data frame when you no longer need it to be grouped because you can get unintended results when using other functions on it. You can ungroup the data frame by using the ungroup function within the dplyr package.\n\n# Always best practice to ungroup data after finished with operation\ndf_wq_c %&gt;% \n  group_by(Station, Year) %&gt;% \n  summarize(\n    min_Chla = min(Chla),\n    mean_Chla = mean(Chla),\n    median_Chla = median(Chla),\n    max_Chla = max(Chla),\n    sd_Chla = sd(Chla)\n  ) %&gt;% \n  ungroup()\n\n# A tibble: 6 × 7\n  Station  Year min_Chla mean_Chla median_Chla max_Chla sd_Chla\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 D7       2020     0.5       1.53        1.74     2.79   0.749\n2 D7       2021     1.17      3.58        3.12     6.76   1.97 \n3 D7       2022     0.67      2.88        2.51     7.36   1.87 \n4 P8       2020     0.64      4.48        2.81    16.5    4.88 \n5 P8       2021     1.25      3.18        3.24     5.56   1.57 \n6 P8       2022     0.57      2.70        2.58     5.24   1.54 \n\n\nNow, you see that the output data frame is no longer grouped. A useful trick is to use the .by argument within summarize to temporarily group the data frame just for the summarize operation.\n\n# It's possible to group data within `summarize`\ndf_wq_c %&gt;% \n  summarize(\n    min_Chla = min(Chla),\n    mean_Chla = mean(Chla),\n    median_Chla = median(Chla),\n    max_Chla = max(Chla),\n    sd_Chla = sd(Chla),\n    .by = c(Station, Year)\n  ) \n\n# A tibble: 6 × 7\n  Station  Year min_Chla mean_Chla median_Chla max_Chla sd_Chla\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 P8       2020     0.64      4.48        2.81    16.5    4.88 \n2 D7       2020     0.5       1.53        1.74     2.79   0.749\n3 P8       2021     1.25      3.18        3.24     5.56   1.57 \n4 D7       2021     1.17      3.58        3.12     6.76   1.97 \n5 D7       2022     0.67      2.88        2.51     7.36   1.87 \n6 P8       2022     0.57      2.70        2.58     5.24   1.54 \n\n\nWe won’t cover it here, but the group_by function also works with other functions in the dplyr package including mutate, filter, and arrange. There are also many other useful things you can do with the summarize function that we won’t cover in this class including using it with the across function. The across function allows for the summarize and mutate functions to apply operations across multiple columns in a data frame. Using it in combination with tidyselect functions allows for much more efficient code.\n\n\n\n\n\n\nSummarize vs. Mutate\n\n\n\nYou may be wondering how summarize and mutate are different since they both do similar things. The main difference is that mutate always returns the same number of rows in the data frame, and summarize returns just one row for the specified summary function(s). summarize with group_by returns a row for each combination of grouping variables.",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "code/Day2_04_Summarize.html#pivoting",
    "href": "code/Day2_04_Summarize.html#pivoting",
    "title": "\n7  Summarizing Data\n",
    "section": "\n7.3 Pivoting",
    "text": "7.3 Pivoting\nLet’s look at our column headers again:\n\ncolnames(df_wq)\n\n [1] \"Station\"            \"Date\"               \"Chla\"              \n [4] \"Pheophytin\"         \"TotAlkalinity\"      \"DissAmmonia\"       \n [7] \"DissNitrateNitrite\" \"DOC\"                \"TOC\"               \n[10] \"DON\"                \"TotPhos\"            \"DissOrthophos\"     \n[13] \"TDS\"                \"TSS\"                \"TKN\"               \n[16] \"Depth\"              \"Secchi\"             \"Microcystis\"       \n[19] \"SpCndSurface\"       \"WTSurface\"         \n\n\nWe note the structure here: we have metadata for Station and Date, and then each of the 18 analytes has its own columns. Therefore, our dimensions are 62 rows x 20 columns. Sometimes, however, it’s more advantageous to structure the data in a different way. For example, maybe we want all analyte names in one column with their values in another.\nWe can achieve this by pivoting the rows and columns. tidyverse has two handy functions for this: pivot_longer and pivot_wider.\n\n7.3.1 pivot_longer\n\nWhen we gather multiple columns into two columns (one for names, another for values), we make the dataset longer; hence, pivot_longer. For this function, you need to specify:\n\nwhich columns to include (cols)\nwhat to call the name column (names_to)\nwhat to call the value column (values_to)\n\nLets use this to pivot all the analytes into two columns. I could achieve this by writing out all the column names. However, a handy shortcut is to use the - operator; similar to negate, this tells the code to consider all columns but the ones included:\n\ndf_wq_long &lt;- df_wq %&gt;%\n  pivot_longer(\n    cols = -c(Station, Date),\n    names_to = 'Analyte',\n    values_to = 'Value'\n  )\n\n\ndf_wq_long\n\n# A tibble: 1,116 × 4\n   Station Date       Analyte            Value\n   &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1 P8      2020-01-16 Chla                0.64\n 2 P8      2020-01-16 Pheophytin          0.5 \n 3 P8      2020-01-16 TotAlkalinity      98   \n 4 P8      2020-01-16 DissAmmonia         0.15\n 5 P8      2020-01-16 DissNitrateNitrite  2.8 \n 6 P8      2020-01-16 DOC                 3.9 \n 7 P8      2020-01-16 TOC                 4.1 \n 8 P8      2020-01-16 DON                NA   \n 9 P8      2020-01-16 TotPhos             0.31\n10 P8      2020-01-16 DissOrthophos       0.2 \n# ℹ 1,106 more rows\n\n\nWe can see that our data shape has changed; we have fewer columns and many more rows:\n\ndim(df_wq_long)\n\n[1] 1116    4\n\n\n\n7.3.2 pivot_wider\n\nWe see that we went from 20 columns to 4. However, we also went from 62 rows to 1116. Why 1116? The columns we did not specify (Station and Date) were used to help create a key; every row is a unique combination of Station, Date, and our new name column Analyte. There are 62 unique Date/Station combos, and each of the 18 analytes has a row for each one, leading to 1116 rows.\nLets pretend this was the original format we received the data in, and we rather have each analyte be its own column. In other words, our goal is to transform two columns into &gt;2 columns, making the dataset wider. The best function for this, as you might suspect, is pivot_wider. Here, instead of pivoting to columns, we pivot from them, and need:\n\nwhich column to take the names from (names_from)\nwhich column to take the values from (values_from)\n\n\ndf_wq_wide &lt;- df_wq_long %&gt;%\n  pivot_wider(\n    names_from = Analyte,\n    values_from = Value\n  )\n\n\ndf_wq_wide\n\n# A tibble: 62 × 20\n   Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 P8      2020-01-16  0.64       0.5             98        0.15\n 2 D7      2020-01-22  0.67       0.87            82        0.21\n 3 P8      2020-02-14  1.46       0.69            81        0.25\n 4 D7      2020-02-20  2.15       0.5             86        0.14\n 5 P8      2020-03-03  1.4        0.56            80        0.11\n 6 D7      2020-03-06  1.89       1.13            93        0.22\n 7 P8      2020-06-11  4.73       1.25            59        0.05\n 8 D7      2020-06-17  1.74       0.89            78        0.05\n 9 P8      2020-07-13  6.4        0.88            63        0.05\n10 D7      2020-07-16  2.79       0.85            80        0.05\n# ℹ 52 more rows\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\n\n\ndim(df_wq_wide)\n\n[1] 62 20\n\n\nNow we’re back to our original dimensions!\nNote: The pivot functions are very powerful and have a large amount of flexibility. For example, you can specify multiple value columns or combine columns to create a single names column. The help documentation has useful examples to reference for cases like these.\n\n7.3.3 Exercise\nNow its your turn to try out grouping and summarizing.\n\nUse group_by and summarize to calculate the minimum, median, and maximum values for “Secchi” for each station.HINT: Watch out for NA values!\nAdd “Year” as a grouping variable to the operation above to calculate summary statistics for “Secchi” for each station and year combination. Assign this as it’s own object named df_summary. HINT: Don’t forget to ungroup your output data frame!\nPivot df_summary so that the secchi statistics are rows and the years are columns. HINT: You will use both pivot longer and pivot wider\n\nClick below for the answer when you are done!\n\nCode# Calculate the minimum, median, and maximum values for Secchi for each station\ndf_wq %&gt;% \n  group_by(Station) %&gt;% \n  summarize(\n    min_Secchi = min(Secchi, na.rm = TRUE),\n    median_Secchi = median(Secchi, na.rm = TRUE),\n    max_Secchi = max(Secchi, na.rm = TRUE)\n  )\n\n# Add \"Year\" as a grouping variable to the operation above\ndf_summary &lt;- df_wq %&gt;% \n  mutate(Year = year(Date)) %&gt;% \n  group_by(Station, Year) %&gt;% \n  summarize(\n    min_Secchi = min(Secchi, na.rm = TRUE),\n    median_Secchi = median(Secchi, na.rm = TRUE),\n    max_Secchi = max(Secchi, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\n# Pivot the data; first make the secchi columns into rows, then the year rows into columns\ndf_summary_long &lt;- df_summary %&gt;%\n  pivot_longer(cols = c('min_Secchi','median_Secchi','max_Secchi'),\n               names_to = 'SecchiStat',\n               values_to = 'Value')\n\ndf_summary_wide &lt;- df_summary_long %&gt;%\n  pivot_wider(names_from = Year, values_from = Value)\n\n# Can also complete in one step\ndf_summary_wide &lt;- df_summary %&gt;%\n  pivot_longer(cols = starts_with('min_'):starts_with('max_'),\n               names_to = 'SecchiStat',\n               values_to = 'Value') %&gt;%\n  pivot_wider(names_from = Year, values_from = Value)",
    "crumbs": [
      "Day 2 - Manipulating Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html",
    "href": "code/Day3_MakingGraphs.html",
    "title": "\n8  Making Graphs\n",
    "section": "",
    "text": "8.1 Introduction\nOne of the greatest strengths of R is its ability to produce a wide variety of professional quality custom graphs. Today, we will cover the basics of visualizing data using the popular ggplot2 package. We will cover a broad range of graphs that are useful for visualizing water quality and biological data.\nAs we decide what type of graph to make, it is useful to consider the types of data we are plotting. In general, continuous data are those comprised of numbers (e.g., water temperature). Categorical data are those that fall into discrete groups or types (e.g., station).\nWhen you download R, there is a suite of packages that are included. These packages include functions for carrying out a variety of basic tasks, including making graphs. I used base R graphing functions for a number of years, and there’s nothing wrong with using them. However, in more recent years, I have completely switched to using the ggplot2 package and think it is a great package for new R users to know.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#set-up",
    "href": "code/Day3_MakingGraphs.html#set-up",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.2 Set up",
    "text": "8.2 Set up\n\n8.2.1 Load the required R packages\nSo without further ado, let’s load the packages we need for today’s tutorial. We will load the tidyverse because it included ggplot2 and a variety of other useful packages. We will also load here to simplify reading and writing files.\nToward the end of our lesson, I will briefly show some features of the patchwork package, which is used for making multi-panel plots, but you don’t need to worry about downloading it yourself right now.\n\nlibrary(tidyverse) #suite of data science tools\nlibrary(here) #setting working directory\nlibrary(patchwork) #combining plots into panels\n\n\n8.2.2 Read in the dataset\nWe will continue to use the same EMP water quality data set that you used for the first two parts of this course.\n\nwqdata &lt;- read_csv( here(\"data/WQ_P8D7.csv\"))\n\n\n8.2.3 Look at the data set\nLet’s take a quick look at the structure of the data to remind ourselves of the column names and data types in this data set.\n\nglimpse(wqdata)\n\nRows: 62\nColumns: 20\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2020-01-16, 2020-01-22, 2020-02-14, 2020-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.64, 0.67, 1.46, 2.15, 1.40, 1.89, 4.73, 1.74, 6.4…\n$ Pheophytin         &lt;dbl&gt; 0.50, 0.87, 0.69, 0.50, 0.56, 1.13, 1.25, 0.89, 0.8…\n$ TotAlkalinity      &lt;dbl&gt; 98.0, 82.0, 81.0, 86.0, 80.0, 93.0, 59.0, 78.0, 63.…\n$ DissAmmonia        &lt;dbl&gt; 0.150, 0.210, 0.250, 0.140, 0.110, 0.220, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 2.800, 0.490, 1.700, 0.480, 1.600, 0.380, 1.070, 0.…\n$ DOC                &lt;dbl&gt; 3.90, 0.27, 2.80, 0.39, 2.00, 0.19, 2.80, 1.20, 3.1…\n$ TOC                &lt;dbl&gt; 4.10, 0.32, 2.50, 0.41, 2.10, 0.20, 2.80, 1.20, 3.1…\n$ DON                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0.30, 0.20, 0.30, 0.10, 0.5…\n$ TotPhos            &lt;dbl&gt; 0.310, 0.082, 0.130, 0.130, 0.190, 0.100, 0.188, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.200, 0.071, 0.130, 0.065, 0.140, 0.082, 0.177, 0.…\n$ TDS                &lt;dbl&gt; 380, 9500, 340, 5800, 290, 8700, 280, 7760, 227, 11…\n$ TSS                &lt;dbl&gt; 8.9, 38.0, 2.2, 18.0, 1.4, 28.0, 6.6, 35.6, 5.3, 23…\n$ TKN                &lt;dbl&gt; 0.520, 0.480, 0.430, 0.250, 0.400, 0.200, 0.400, 0.…\n$ Depth              &lt;dbl&gt; 28.9, 18.8, 39.0, 7.1, 39.0, 7.2, 37.1, 5.2, 36.7, …\n$ Secchi             &lt;dbl&gt; 116, 30, 212, 52, 340, 48, 100, 40, 160, 44, 120, 6…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 3, 2, 3, 2, 4, 2, 3, 2, 2, 1, 1, …\n$ SpCndSurface       &lt;dbl&gt; 667, 15532, 647, 11369, 530, 16257, 503, 12946, 404…\n$ WTSurface          &lt;dbl&gt; 9.67, 9.97, 11.09, 12.51, 13.97, 13.81, 23.46, 21.1…\n\n\nJust about everything is “dbl” or “double”, which just means they are considered numeric data. As exceptions, Station is character, and Date is date.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#histograms",
    "href": "code/Day3_MakingGraphs.html#histograms",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.3 Histograms",
    "text": "8.3 Histograms\nLet’s start making graphs! The ggplot2 package utilizes the grammar of graphics and creates plots in “layers”. We will learn about some of the core “geometries”, which specify how the graph will look (e.g., bar plot vs line plot).\nWe will begin by making a simple histogram, so we can learn the anatomy of the ggplot() function. Histograms show the distribution of data for a single variable (categorical or continuous). We will plot surface water temperature (WTSurface).\nWithin the ggplot() function, the first thing we should do is specify the data set we want to plot using the data argument. Then, we will indicate which column within the data set we want to plot. These are the most basic components.\nFrom here, we use the + sign to add layers upon the foundation we have built. In this case, we want a histogram, so we use the argument geom_histogram(). There are many types of graphs you can make and ggplot has similarly named arguments for each of them (e.g., geom_point, geom_line). We will look at more of these shortly.\nI surround my plot code with parentheses, so the plots render immediately when I run the code. If you don’t include those parentheses, running the plotting code will simply save the plot as the named object, which may be all you want in some cases.\n\n(plot_hist_temp &lt;- ggplot(data = wqdata, aes(x = WTSurface)) +\n  geom_histogram()\n)\n\n\n\n\n\n\n\nWater temperature is a continuous variable, so the widths of the histogram bins are a bit arbitrary. R made a guess about how many bins to use, but this guess doesn’t work very well. We can make some manual adjustments within the geom_histogram() argument. Let’s try reducing the number of bins to eight.\n\n(plot_hist_temp_bins &lt;- ggplot(data = wqdata, aes(x = WTSurface)) +\n  geom_histogram(bins = 8)\n)\n\n\n\n\n\n\n\n\n8.3.1 Your Turn\nBased on the code above for the water temperature histogram, make a histogram of surface specific conductance (SpCndSurface) and try different numbers of bins.\n\nCode(plot_hist_sc &lt;- ggplot(data = wqdata, aes(x = SpCndSurface)) +\n   geom_histogram(bins = 6)\n)\n\n\nIf we had plotted data for a single station, the histogram would have been roughly bell shaped, but because we are plotting data for two stations in different parts of the salinity gradient, we get a bimodal distribution.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#time-series-plots",
    "href": "code/Day3_MakingGraphs.html#time-series-plots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.4 Time series plots",
    "text": "8.4 Time series plots\nNext, let’s plot a time series of data. These plots are helpful for looking at monitoring data through time to identify patterns or for finding outliers. For a histogram, there was only an x variable. For most types of plots, including time series plots, there are x and y variables to specify.\nWe will plot a year of Secchi depth data to see how it changes through the seasons. As a reminder, higher Secchi depth means lower turbidity.\nFor simplicity, we will just plot the data for one station, P8. Each value for Secchi will be plotted as a point, so we will use geom_point().\n\n#filter data set to only data for station P8\nwqdata_p8 &lt;- wqdata %&gt;% \n  filter(Station == \"P8\") \n\n#filter data set to only data for station D7\n#we will use this a little later\nwqdata_d7 &lt;- wqdata %&gt;% \n  filter(Station == \"D7\") \n\n#now plot the P8 subset of the data\n(plot_ts_points &lt;- ggplot(data = wqdata_p8, aes(x = Date, y = Secchi)) +\n  geom_point()\n)\n\n\n\n\n\n\n\n\n#you can also do the filtering and plotting all at once if you want\n(plot_ts_points2 &lt;- wqdata %&gt;% \n    filter(Station == \"P8\") %&gt;% \n      ggplot(aes(x = Date, y = Secchi)) +\n    geom_point()\n)\n\n\n\n\n\n\n\nWe can start to see how Secchi depth changed over time, but it would be easier with lines connecting the points. Let’s add some, which will highlight how ggplot works by building layers. As you might expect, we add the lines using geom_line().\n\n#time series plot with connecting lines\n(plot_ts_points_lines_p8 &lt;- ggplot(data = wqdata_p8, aes(x = Date, y = Secchi)) +\n  geom_point()+\n  geom_line())\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nBecause we are simply building upon the previous graph made with only the points, we can also use this shortened approach, which further demonstrates that we are simply adding layers.\n\n(plot_ts_points_lines_p8_2&lt;- plot_ts_points +\n  geom_line())\n\n\n\n\n\n\n\nNow it is a little easier to see the patterns through time. We can see that Secchi depth varies seasonally, generally with low values in rainy winter months and high values in dry summer months. Note there is a gap in the time series in 2021 to reflect a missing value.\n\n8.4.1 Your Turn\nMake a similar time series plot of Secchi depth for station D7 (use wqdata_D7 as the data set). How does Secchi compare between the two stations?\n\nCode(plot_ts_points_lines_d7 &lt;- ggplot(data = wqdata_d7, aes(x = Date, y = Secchi)) +\n  geom_point()+\n  geom_line())\n\n\nSecchi depth is generally much lower at D7 than P8, indicating higher turbidity at D7.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#multiple-time-series-per-plot",
    "href": "code/Day3_MakingGraphs.html#multiple-time-series-per-plot",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.5 Multiple time series per plot",
    "text": "8.5 Multiple time series per plot\nWe can plot multiple time series on the same graph in order to more easily compare them. We will plot Secchi for P8 and D7 together.\nTo include both stations as separate lines, we use group = Station. To get different colors for the points and lines by station, we include color = Station within each geom. These are just default colors. We will see how to specify our own colors later.\n\n#time series plot with connecting lines for both stations\n(plot_ts_points_lines_two &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi, group = Station)) +\n  geom_point(aes(color = Station))+\n  geom_line(aes(color = Station)))\n\n\n\n\n\n\n\nThis plot clearly shows what we noted above in the individual plots of Secchi depth, namely that Secchi depth is generally much higher at P8 than D7.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#scatterplots",
    "href": "code/Day3_MakingGraphs.html#scatterplots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.6 Scatterplots",
    "text": "8.6 Scatterplots\nNext, let’s make a scatterplot. These plots are great for looking at relationships between pairs of continuous variables. As an example, we will plot dissolved organic carbon against total organic carbon. We will go back to using the dataset with both stations.\n\n(plot_scatter &lt;- ggplot(data = wqdata, aes(x = DOC, y = TOC)) +\n  geom_point()\n)\n\n\n\n\n\n\n\nThere appears to be a strong relationship between these two variables. Let’s add a trend line. As with the time series plot, we are building layers. However, we are not simply adding lines to connect the points, so we aren’t using geom_line(). To plot a trend line, we use geom_smooth(). There are various smooths we could apply, so we need to specify what we want. In this case, we use a linear model (LM). By default, a 95% confidence interval (dark gray) is added around the trend line (blue).\n\n(plot_scatter_trend &lt;- plot_scatter +\n   geom_smooth(method = lm)\n   )\n\n\n\n\n\n\n\nOur focus today is making graphs rather than performing statistics, but I just wanted to show the correlation coefficient for this relationship.\n\ncor.test(wqdata$DOC,wqdata$TOC)\n\n\n    Pearson's product-moment correlation\n\ndata:  wqdata$DOC and wqdata$TOC\nt = 74.274, df = 60, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9910303 0.9967584\nsample estimates:\n      cor \n0.9946058 \n\n\nThe correlation is very strong between TOC and DOC (0.99). By convention, a correlation coefficient between 0.7 and 1.00 (the max) is considered a strong correlation.\n\n8.6.1 Your Turn\nPlot the relationship between Chla and Pheophytin and fit a line to it.\n\nCode(plot_scatter_trend_phyto &lt;- ggplot(data = wqdata, aes(x = Chla, y = Pheophytin)) +\n  geom_point()+\n  geom_smooth(method = lm)\n)\n\n\nThis does not look like much of a relationship. Let’s look at the correlation coefficient.\n\nCodecor.test(wqdata$Chla,wqdata$Pheophytin)\n\n\nThe correlation coefficient is -0.05, which indicates that Chla and Pheophytin are not significantly correlated. Even if we removed the outlier at the high end of Chla, it would still not show a significant relationship.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#box-and-whisker-plots",
    "href": "code/Day3_MakingGraphs.html#box-and-whisker-plots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.7 Box and whisker plots",
    "text": "8.7 Box and whisker plots\nIn scatter plots, we plot a continuous variable against another continuous variable. In some cases, we want to plot a continuous variable against a categorical variable. One type of plot that is useful in this situation is a box and whisker plot. These are great for initial data exploration and sometimes even in publications.\nMicrocystis abundance data is collected using a visual scoring system of 1 - 5, with 5 being the highest.\n\n\nMicrocystis ordinal scores\n\nMicrocystis blooms are associated with high water temperature. Let’s plot the Microcystis scores (categorical) against water temperature (continuous).\n\n(plot_bw1 &lt;- ggplot(data = wqdata, aes(x = Microcystis, y = WTSurface)) +\n   geom_boxplot()\n   )\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\n\n\n\nNote that Microcystis scores are ordered categories (i.e., ordinal), so R read them in as numeric data. If we try to make the plot with numeric Microcystis data, we don’t get the plot we expect. We will make this column a factor to indicate that these numbers are actually categories.\n\n(plot_bw2 &lt;- ggplot(data = wqdata, aes(x = factor(Microcystis), y = WTSurface)) +\n   geom_boxplot()\n   )\n\n\n\n\n\n\n\nThis plot is what we were looking for, and it shows an increase in Microcystis density with water temperature, like we anticipated. Note that scores of four are rare, and scores of five are absent from the data set.\nThe thick horizontal bar in the middle of the box is the median (not the mean). The bottom and top box borders are the 25th percentile (i.e., 1st quartile) and 75th percentiles (i.e., 3rd quartile), respectively. The bottom and top whiskers are the interquartile range (i.e., box height) * 1.5. Points beyond the whiskers are outliers.\nPersonally, I like to include all the raw data points on the box and whisker plots because it gives folks an even better understanding of the distribution of the data. To do this, we add a layer with geom_jitter(). It is called this because the points are “jittered” to minimize overlap among them.\n\n(plot_bw_points &lt;- plot_bw2 +\n   geom_jitter()\n   )\n\n\n\n\n\n\n\n\n8.7.1 Your Turn\nMicrocystis needs a lot of light in addition to warm water. Plot Microcystis against Secchi depth to see whether turbidity is related to Microcystis density. Include the points for the raw data.\n\nCode(plot_bw_turb &lt;- ggplot(data = wqdata, aes(x = factor(Microcystis), y = Secchi)) +\n   geom_boxplot() +\n   geom_jitter()\n   )\n\n\nThe pattern is not as stark as for temperature, but we do see that Microcsystis scores are higher when Secchi depth is greater.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#barplots",
    "href": "code/Day3_MakingGraphs.html#barplots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.8 Barplots",
    "text": "8.8 Barplots\nBar plots are similar to box and whisker plots but show less information. They generally include the mean and perhaps an indicator of variation.\nWe will make bar plots that are analogous to the box and whisker plots above. We will need to calculate some summary statistics (mean, standard deviation) for the continuous variables by Microcystis score before we can make the plots.\n\nwqdata_means &lt;- wqdata %&gt;% \n  group_by(Microcystis) %&gt;% \n  summarize(temp_mean = mean(WTSurface)\n            ,temp_sd = sd(WTSurface)\n            ,secchi_mean = mean(Secchi,na.rm = T)\n            ,secchi_sd = sd(Secchi,na.rm = T)\n            )\n\n(plot_bar &lt;- ggplot(data = wqdata_means, aes(x = factor(Microcystis), y = temp_mean)) +\n   geom_bar(stat = 'identity')\n   )\n\n\n\n\n\n\n\nHere’s another way to make the same plot.\n\n(plot_bar2 &lt;- ggplot(data = wqdata_means) +\n    geom_col(aes(x = factor(Microcystis), y = temp_mean))\n    )\n\n\n\n\n\n\n\nAdding error bars to bar plots provides more information about the distribution of the data, a bit like the whiskers in the box and whisker plots above. There are different options for types of error bars (e.g., standard deviation, standard error, 95% confidence interval). We will add error bars to our Microcystis plot based on standard deviation.\n\n(plot_bar_error &lt;- ggplot(data = wqdata_means, aes(x = factor(Microcystis), y = temp_mean)) +\n   geom_bar(stat = 'identity')+\n     geom_errorbar(aes(ymin = temp_mean-temp_sd, ymax = temp_mean+temp_sd),width = 0.2)\n   )\n\n\n\n\n\n\n\nWith the addition of error bars, we can see there is more variation in temperature for Microcystis score 1 than for Microcystis score 4.\n\n8.8.1 Your Turn\nMake bar plot of Microcystis scores vs Secchi depth. Include standard error bars. This is analogous to the box and whisker plot you made previously.\n\nCode(plot_bar_error_secchi &lt;- ggplot(data = wqdata_means, aes(x = factor(Microcystis), y = secchi_mean)) +\n   geom_bar(stat = 'identity')+\n     geom_errorbar(aes(ymin = secchi_mean-secchi_sd, ymax = secchi_mean+secchi_sd),width = 0.2)\n   )",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#stacked-bar-plot",
    "href": "code/Day3_MakingGraphs.html#stacked-bar-plot",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.9 Stacked bar plot",
    "text": "8.9 Stacked bar plot\nSometimes we want a bar plot that simultaneously shows a total and its component parts at the same time, which is where stacked bar plots are useful.\nLet’s go back to the subset of data for Station P8. We might want to look at Dissolved Ammonia (DissAmmonia) and Dissolved Nitrate and Nitrite (DissNitrateNitrite) through time together.\n\n#first we will rearrange the data frame a bit to make plotting easier\nwqdata_p8_nutrients &lt;- wqdata_p8 %&gt;% \n  #reduce data set down to just the needed columns\n  select(Station, Date, DissAmmonia, DissNitrateNitrite) %&gt;% \n  #convert from wide to long format\n  pivot_longer(cols = c(DissAmmonia,DissNitrateNitrite), names_to = \"Nutrient\", values_to = \"Value\")\n\n\n(plot_bar_stack_abs &lt;- ggplot(data = wqdata_p8_nutrients, aes(x = Date, y = Value, fill = Nutrient)) +\n   geom_bar(stat = 'identity')\n   )\n\n\n\n\n\n\n\nPerhaps we are more interested in the relative values rather than the absolute values. We just make a minor modification to the code above to accomplish this.\n\n(plot_bar_stack_rel &lt;- ggplot(data = wqdata_p8_nutrients, aes(x = Date, y = Value, fill = Nutrient)) +\n   geom_bar(stat = \"identity\", position = \"fill\")\n )\n\n\n\n\n\n\n\n\n8.9.1 Your Turn\nMake a stacked bar plot showing the relative amounts of ammonia and nitrate plus nitrite for D7 using the data set below (i.e., wqdata_d7_nutrients). How does the proportion of ammonia at D7 compare to that of P8?\n\nwqdata_d7_nutrients &lt;- wqdata_d7 %&gt;% \n  #reduce data set down to just the needed columns\n  select(Station, Date, DissAmmonia, DissNitrateNitrite) %&gt;% \n  #convert from wide to long format\n  pivot_longer(cols = c(DissAmmonia,DissNitrateNitrite), names_to = \"Nutrient\", values_to = \"Value\")\n\n\nCode(plot_bar_stack_rel_d7 &lt;- ggplot(data = wqdata_d7_nutrients, aes(x = Date, y = Value, fill = Nutrient)) +\n   geom_bar(stat = \"identity\", position = \"fill\")\n)",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#faceted-plots",
    "href": "code/Day3_MakingGraphs.html#faceted-plots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.10 Faceted plots",
    "text": "8.10 Faceted plots\nSometimes we want to show the same type of plot for multiple groups at once, so we can compare them. It is easy to make multi-panel plots in ggplot2 using facets.\nAbove, we plotted Secchi depth for both stations on the same plot. Sometimes, it is more useful to create a two panel plot, in which each time series is plotted on a separate graph.\n\n#faceted time series plot for Dissolved Ammonia\n(plot_ts_points_lines_facet &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point()+\n  geom_line())+\n  facet_grid(Station~.)\n\n\n\n\n\n\n\n\n8.10.1 Your Turn\nMake a faceted plot of nutrients for station P8 using the dataset wqdata_p8_nutrients. Show the absolute amounts rather than the relative amounts. Show the two facets of the time series stacked on top of each other like the plot above for Secchi depth.\n\nCode#time series plot for two analytes\n(plot_bar_stack_abs_facet &lt;- ggplot(data = wqdata_p8_nutrients, aes(x = Date, y = Value)) +\n   geom_bar(stat = 'identity')+\n  facet_grid(Nutrient~.)\n)\n\n\nBy default, the y-axis range is standardized across the panels. This is generally a good idea. However, we can change this if we like. Ammonia concentrations are much lower than nitrate plus nitrite concentrations, which makes ammonia concentrations difficult to read on the plot. Let’s use different y-axis ranges for the two analytes to fix that.\n\n#time series plot for two analytes with different y-axis ranges\n(plot_bar_stack_abs_facet &lt;- ggplot(data = wqdata_p8_nutrients, aes(x = Date, y = Value)) +\n   geom_bar(stat = 'identity')+\n  facet_grid(Nutrient~., scales = \"free_y\")\n)",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#combined-plots",
    "href": "code/Day3_MakingGraphs.html#combined-plots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.11 Combined plots",
    "text": "8.11 Combined plots\nSometimes, it is useful to combine multiple plots into a panel that do not share a common x or y axis (i.e., are not produced using facet_grid() or facet_wrap()). In the past, this was difficult to do, but the patchwork package makes is very easy. These plot panels are particularly useful in publications like journal articles.\n\n#two side by side\nplot_hist_temp_bins + plot_bar_stack_abs\n\n\n\n\n\n\n\n\n#two stacked\nplot_hist_temp_bins / plot_bar_stack_abs\n\n\n\n\n\n\n\n\n#one large on left and two small stacked on right\nplot_hist_temp_bins+(plot_hist_temp_bins / plot_bar_stack_abs)",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#customizing-plots",
    "href": "code/Day3_MakingGraphs.html#customizing-plots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.12 Customizing plots",
    "text": "8.12 Customizing plots\nAs I mentioned at the outset, one of the greatest advantages of using R to make graphs is that you can customize them endlessly. We will touch on just a few common types of customizations.\nWe will use the Secchi depth time series again. Previously, we used color to help differentiate the two stations, but we just used the default colors. We can specify the exact colors we want. We can also change the size, shape, and color of the points and the type, width, and color of the lines.\nIt is best to not rely solely on color for differentiating groups in plots because they can be hard to distinguish by folks with color blindness. I recommend using different point types and/or line types in addition to, or instead of, colors for this reason.\n\n8.12.1 Quick references for the customizations options\nColors: https://r-charts.com/colors/\nPoint shapes: \nLine types: \nPlot themes: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n#basic time series plot with connecting lines for both stations\n(plot_ts_points_lines_two &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point(aes(color = Station))+\n  geom_line(aes(color = Station)))\n\n\n\n\n\n\n#different point types for different stations\n(plot_ts_points_lines_two_pts &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point(aes(color = Station, shape = Station))+\n  geom_line(aes(color = Station)))\n\n\n\n\n\n\n#different line types for different stations\n(plot_ts_points_lines_two_pts_lty &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point(aes(color = Station, shape = Station))+\n  geom_line(aes(color = Station, linetype = Station)))\n\n\n\n\n\n\n#adjust point sizes and line widths\n(plot_ts_points_lines_two_pts_lty_sz &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point(aes(color = Station, shape = Station), size = 3)+\n  geom_line(aes(color = Station, linetype = Station), size = 1.2))\n\n\n\n\n\n\n#Customize colors \n(plot_ts_points_lines_two_custom &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point(aes(color = Station, shape = Station), size = 3)+\n  geom_line(aes(color = Station, linetype = Station), size = 1.2)+\n  scale_color_manual(values =c(\"midnightblue\",\"darkorange3\"))\n)\n\n\n\n\n\n\n#Customize point shapes \n(plot_ts_points_lines_two_custom2 &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point(aes(color = Station, shape = Station), size = 3)+\n  geom_line(aes(color = Station, linetype = Station), size = 1.2)+\n  scale_color_manual(values =c(\"midnightblue\",\"darkorange3\")) +\n  scale_shape_manual(values = c(15,17))\n)\n\n\n\n\n\n\n#Customize line types\n(plot_ts_points_lines_two_custom3 &lt;- ggplot(data = wqdata, aes(x = Date, y = Secchi)) +\n  geom_point(aes(color = Station, shape = Station), size = 3)+\n  geom_line(aes(color = Station, linetype = Station), size = 1.2)+\n  scale_color_manual(values =c(\"midnightblue\",\"darkorange3\")) +\n  scale_shape_manual(values = c(15,17)) +\n  scale_linetype_manual(values = c(\"dotted\",\"longdash\"))\n)\n\n\n\n\n\n\n\nSo far, we have used the default color palette in ggplot2 as well as some customized colors. There is also a variety of R packages with thoughtfully developed color palettes. One example is the viridis package, which includes palettes that are both attractive and color blind friendly. It is so popular it is now built into ggplot2.\nOur Sechi depth plot isn’t a great example for showing color palettes, so we will go back to our Microcystis scores plot.\n\n(plot_bar &lt;- ggplot(data = wqdata_means, aes(x = factor(Microcystis), y = temp_mean, fill = factor(Microcystis))) +\n   geom_bar(stat = 'identity')+\n   scale_fill_viridis_d()\n   )\n\n\n\n\n\n\n\nThere are also options for formatting the plot background. Many people dislike the default gray background with grid lines. We will look at a couple of the alternatives.\n\n#change background from gray with white grid to white with gray grid\n(plot_background_bw &lt;- plot_ts_points_lines_two_custom + \n  theme_bw()\n)\n\n\n\n\n\n\n#change background to white with no grid and only axis lines on the left and bottom sides\n(plot_background_classic &lt;- plot_ts_points_lines_two_custom + \n  theme_classic()\n)\n\n\n\n\n\n\n\nIn addition, we can customize the names used to label the x-axis and y-axis and add plot titles. It is important to provide good labels for your plot axes. In particular, make sure to include the measurement units when relevant. I don’t personally use plot titles much, but they can quickly inform your audience about the purpose of the plot.\n\n(plot_labels &lt;- plot_background_classic +\n  labs(x = \"Sampling Date\"\n       , y = \"Secchi depth (cm)\"\n       ,title = \"Secchi depth comparison\"\n       )\n)\n\n\n\n\n\n\n\nNow our plot is looking pretty good and could be published in a report or journal.\n\n8.12.2 Your Turn\nUsing the Secchi depth plot, try out different point shapes, line types, and color combinations.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day3_MakingGraphs.html#saving-plots",
    "href": "code/Day3_MakingGraphs.html#saving-plots",
    "title": "\n8  Making Graphs\n",
    "section": "\n8.13 Saving plots",
    "text": "8.13 Saving plots\nIn RStudio, you can right click on a plot to copy it and then paste it elsewhere (e.g., email, word document), which works fine as a quick and dirty approach. However, a better way to export a plot involves using the ggsave() function. We will export our customized Secchi depth comparison plot.\n\nggsave(plot = plot_labels #tell ggsave which plot to export\n       , filename = \"images/secchi_depth_comp.png\" #provide the name for the image file\n       , width = 6, height =4, units = \"in\" #dimensions of the exported image\n       , dpi = 300 #resolution of the image (dots per inch)\n       )\n\nI used PNG, which is a file type that works well for viewing on a computer. However, ggsave() can save plots as PDF, JPEG, TIFF, and various others. Search for ggsave() within ‘Help’ to see the full list of file types.\nBy default, the plot will be saved in your working directory. I specified a subfolder (i.e., “images”) within my working directory.",
    "crumbs": [
      "Day 3 - Visualizing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Graphs</span>"
    ]
  },
  {
    "objectID": "code/Day4_01_Outline.html",
    "href": "code/Day4_01_Outline.html",
    "title": "Day 4 - Binds, Joins, Dates and Times",
    "section": "",
    "text": "Now that we have covered the basics of using R and RStudio including importing, cleaning, summarizing and plotting data, we’ll continue with a few additional and useful beginner/intermediate skills. Here is the outline for our last day of training:\n\nDates and Times\n\nBasics and Classes\nConverting from character dates\nTime Zones\nExtracting parts of a date-time object\n\nBinds and Joins\n\nDifferences and Usage\nBind Rows\nJoin Basics\nLeft Join\n\nFinal Course Exercise\n\nStudents will have the opportunity to bring many of the class concepts together in this final exercise.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html",
    "href": "code/Day4_02_Dates_and_Times.html",
    "title": "\n9  Dates and Times\n",
    "section": "",
    "text": "9.1 Introduction\nDates and times are one of the more challenging data classes to deal with. There are many different ways of storing date and time information, and it’s easy to get them mixed up. Fortunately, there are a number of tricks that help make things easier, most of which are in the package lubridate, which is part of the tidyverse.\nFor more info, see the Dates and Times chapter in R for Data Science.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html#datetime-basics",
    "href": "code/Day4_02_Dates_and_Times.html#datetime-basics",
    "title": "\n9  Dates and Times\n",
    "section": "\n9.2 Date/Time Basics",
    "text": "9.2 Date/Time Basics\nFirst, let’s look at dates. Dates are stored as the number of days since the origin (default origin is 1970-01-01). We can use the today function to get a date with today’s value.\n\nlibrary(tidyverse)\n\ntoday()\n\n[1] \"2025-07-23\"\n\nas.numeric(today())\n\n[1] 20292\n\n\nNow let’s talk about date-times. R automatically stores the dates and times together as the number of seconds since the origin.\n\nnow()\n\n[1] \"2025-07-23 15:18:39 PDT\"\n\nas.numeric(now())\n\n[1] 1753309119\n\n\nNote that R automatically set the time in Pacific Daylight time since that is what the operating system of this computer uses.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html#datetime-classes",
    "href": "code/Day4_02_Dates_and_Times.html#datetime-classes",
    "title": "\n9  Dates and Times\n",
    "section": "\n9.3 Date/Time Classes",
    "text": "9.3 Date/Time Classes\nIf you have a list of dates and times that you are importing into R, the read_csv or read_excel functions can usually guess when they are dates, particularly if they are in YYYY-MM-DD format, but sometimes they get confused. They have an especially hard time if they are in a non-standard format, such as MM/DD/YY. In this case, the field will get read in as a character and you have to convert it to a date/time class, usually POSIXct.\nBefore we do that, let’s learn a bit about different types of date/time classes.\nDate - Number of days since origin.\nPOSIXct - Number of seconds since origin. This is the most common format.\nPOSIXlt - List of vectors with components sec, min, hour, mday, mon, and year. You probably won’t use this format very often.\nPOSIXct is more convenient for including in data frames, and POSIXlt is closer to human-readable forms. Both classes may have an attribute tzone, specifying the time zone. See ?DateTimeClasses for details.\nNote: There is no built-in “Time” class, only “DateTime”. Some packages (like hms) have developed ways to deal with times on their own, but it doesn’t come with your base R installation.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html#converting-from-character-dates",
    "href": "code/Day4_02_Dates_and_Times.html#converting-from-character-dates",
    "title": "\n9  Dates and Times\n",
    "section": "\n9.4 Converting from character dates",
    "text": "9.4 Converting from character dates\nThe base R function to convert from characters to dates is strptime\n\n?strptime\n\nTo use this function, we just feed in the date in character form and specify what format it is in. The format syntax is a little annoying, but the documentation for strptime gives you all the options.\n\n# Date in character format\ndate1 &lt;- \"2/3/2021\"\n\n# Convert it. Little m for month, little d for day, capital Y for four-digit year.\ndate1_conv &lt;- strptime(date1, format = \"%m/%d/%Y\")\ndate1_conv\n\n[1] \"2021-02-03 PST\"\n\n\n\n# Try a different format. Lowercase y for two-digit year.\ndate2 &lt;- \"2/3/21\"\nstrptime(date2, format = \"%m/%d/%y\")\n\n[1] \"2021-02-03 PST\"\n\n\n\n# Now one with time included. Note that any spaces or dashes need to be the same \n# in the format call as in your character vector.\ndate3 &lt;- \"2-3-2021 08:15\"\nstrptime(date3, format = \"%m-%d-%Y %H:%M\")\n\n[1] \"2021-02-03 08:15:00 PST\"\n\n\nThe format strings are quite annoying, so fortunately the lubridate package has a number of shortcut functions to make converting easier.\n\n?ymd\n\nLet’s try the dates we converted above with these functions.\n\nmdy(date1)\n\n[1] \"2021-02-03\"\n\nmdy(date2)\n\n[1] \"2021-02-03\"\n\nmdy_hm(date3)\n\n[1] \"2021-02-03 08:15:00 UTC\"\n\n\nNote that strptime defaulted the time zone to PDT, whereas mdy_hm defaulted to UTC. It’s always wise to specify your time zone manually to make sure issues don’t come up.\n\nmdy_hm(date3, tz = \"America/Los_Angeles\")\n\n[1] \"2021-02-03 08:15:00 PST\"\n\n\nNote that when I specified the time zone, I actually specified the location, since we switch time zones with Daylight Savings.\nTo get a complete list of time zone names, see OlsonNames()\n\nOlsonNames()\n\nIf you have a dataset that is collected in California, but doesn’t use Daylight savings, use “Etc/GMT+8”. GMT (Greenwich Mean Time) doesn’t change with daylight savings, and we are 8 hours behind. Most if not all water quality data collected by DWR is recorded in Pacific Standard Time (PST) year-round, so “Etc/GMT+8” is the correct time zone to use for these instances.\n\nmdy_hm(date3, tz = \"Etc/GMT+8\")\n\n[1] \"2021-02-03 08:15:00 -08\"\n\n\n\n9.4.1 Exercise\nNow its your turn to try out working with dates and times.\nCreate a new object for a date and time in a character format. Then, try using strptime and the functions from the lubridate package to convert the object to a date-time object. If you want, try this a few times with different date-time formats. Make sure to specify the time zone.\nClick below for the answer when you are done!\n\nCode# Create a new object for a date and time in a character format\nmy_datetime_chr &lt;- \"7/7/2025 13:00\"\n\n# Convert character object to date-time object using strptime\nstrptime(my_datetime_chr, format = \"%m/%d/%Y %H:%M\")\n\n# Convert character object to date-time object using lubridate package\nmdy_hm(my_datetime_chr, tz = \"America/Los_Angeles\")\n\n# Try this again with a different date-time format\nmy_datetime_chr2 &lt;- \"2025-07-07 13:00:00\"\nstrptime(my_datetime_chr2, format = \"%Y-%m-%d %H:%M:%S\")\nymd_hms(my_datetime_chr2, tz = \"America/Los_Angeles\")\n\n\n\n9.4.2 Heterogeneous formats\nSometimes you will encounter a character vector of date-times with multiple formats. While this is somewhat unusual, it is very annoying, so its good to know how to handle this issue. Fortunately the lubridate package has a function to help with this - parse_date_time(). This function has an “orders” argument that allows the user to provide a vector of more than one date-time format to try parsing.\nFirst lets try to use the strptime and the lubridate functions we previously learned about to parse a character vector of date-times with multiple formats:\n\ndate4 &lt;- c(\"2/3/2021\", \"2021/02/03\")\nstrptime(date4, format = \"%m/%d/%Y\")\n\n[1] \"2021-02-03 PST\" NA              \n\nmdy(date4)\n\n[1] \"2021-02-03\" NA          \n\n\nNotice that only one of the two dates parsed correctly and the other was returned as an NA value. Now lets use the parse_date_time() function:\n\nparse_date_time(date4, orders = c(\"mdY\", \"Ymd\"))\n\n[1] \"2021-02-03 UTC\" \"2021-02-03 UTC\"\n\n\nBoth dates are now parsed correctly.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html#changing-the-date-time-format",
    "href": "code/Day4_02_Dates_and_Times.html#changing-the-date-time-format",
    "title": "\n9  Dates and Times\n",
    "section": "\n9.5 Changing the date-time format",
    "text": "9.5 Changing the date-time format\nR automatically displays dates and times in Year-Month-Day format. This is the international standard. If you want to change the output format, just use the format function. Note that this converts the date back to a character class.\n\ndate1_conv\n\n[1] \"2021-02-03 PST\"\n\nformat(date1_conv, format = \"%m/%d/%y\")\n\n[1] \"02/03/21\"\n\n\nBut really, why would you want to change the format?\n\nhttps://xkcd.com/1179",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html#time-zones",
    "href": "code/Day4_02_Dates_and_Times.html#time-zones",
    "title": "\n9  Dates and Times\n",
    "section": "\n9.6 Time Zones",
    "text": "9.6 Time Zones\nWhen working with date-times in R, its useful to understand how to change or convert the time zone of an object. The lubridate package has a few useful functions for working with time zones - tz, force_tz, and with_tz.\nYou can check the time zone of a date-time object with the tz function.\n\ndate5 &lt;- ymd_hms(\"2025-06-19 14:00:00\")\ntz(date5)\n\n[1] \"UTC\"\n\n\nIf you have a date-time object that is in the wrong time zone, you can use force_tz to change the time zone but keep the same clock time. For example,\n\ndate5\n\n[1] \"2025-06-19 14:00:00 UTC\"\n\ndate5_conv &lt;- force_tz(date5, tzone = \"Etc/GMT+8\")\ndate5_conv \n\n[1] \"2025-06-19 14:00:00 -08\"\n\n\nIf you want to convert a date-time object to an new time zone, converting both the clock time and time zone, you can use the ‘with_tz’ function.\n\ndate5_conv\n\n[1] \"2025-06-19 14:00:00 -08\"\n\nwith_tz(date5_conv, tzone = \"America/Los_Angeles\")\n\n[1] \"2025-06-19 15:00:00 PDT\"\n\n\n\n9.6.1 Exercise\nNow its your turn to try out working with time zones.\nCreate a new object for a date and time as a date-time object. Check its time zone. Then, change its time zone to Pacific Standard Time (“Etc/GMT+8”) keeping it as the same clock time. Next, convert its time zone to the US Eastern time zone changing both the clock time and time zone. Hint: use OlsonNames() to find the time zone name.\nClick below for the answer when you are done!\n\nCode# Create a new object for a date and time\nmy_datetime &lt;- ymd_hms(\"2025-07-07 13:00:00\")\n\n# Check its time zone\ntz(my_datetime)\n\n# Change time zone to PST keeping it as same clock time\nforce_tz(my_datetime, tzone = \"Etc/GMT+8\")\n\n# Change time zone to US Eastern time zone changing both clock time and time zone\nwith_tz(my_datetime, tzone = \"US/Eastern\")",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html#times",
    "href": "code/Day4_02_Dates_and_Times.html#times",
    "title": "\n9  Dates and Times\n",
    "section": "\n9.7 Times",
    "text": "9.7 Times\nAs noted above, there is no built-in time class. Only date/time classes. So, if we have a bunch of times R will automatically add a date to them.\n\ntimes &lt;- c(\"1:20\", \"2:30\", \"3:50\", \"14:00\")\ntimes &lt;- strptime(times, format = \"%H:%M\")\ntimes\n\n[1] \"2025-07-23 01:20:00 PDT\" \"2025-07-23 02:30:00 PDT\"\n[3] \"2025-07-23 03:50:00 PDT\" \"2025-07-23 14:00:00 PDT\"\n\n\nIt will usually default to today’s date, or sometimes to Dec 31st, 1899. If you have a “Date” and a “Time” column in your dataframe, it’s best to just combine them for manipulation.\n\ndf_date_times &lt;- tibble(\n  times = c(\"1:20\", \"2:30\", \"3:50\", \"14:00\"),\n  dates = c(\"2021-01-01\", \"2023-12-01\", \"2011-06-04\", \"2022-10-11\")\n)\n\nglimpse(df_date_times)\n\nRows: 4\nColumns: 2\n$ times &lt;chr&gt; \"1:20\", \"2:30\", \"3:50\", \"14:00\"\n$ dates &lt;chr&gt; \"2021-01-01\", \"2023-12-01\", \"2011-06-04\", \"2022-10-11\"\n\ndf_date_times_c &lt;- df_date_times %&gt;% \n  mutate(\n    Date = ymd(dates), \n    Time = strptime(times, format = \"%H:%M\"),\n    DateTime = ymd_hm(paste(dates, times), tz = \"America/Los_Angeles\")\n  )\n\nglimpse(df_date_times_c)\n\nRows: 4\nColumns: 5\n$ times    &lt;chr&gt; \"1:20\", \"2:30\", \"3:50\", \"14:00\"\n$ dates    &lt;chr&gt; \"2021-01-01\", \"2023-12-01\", \"2011-06-04\", \"2022-10-11\"\n$ Date     &lt;date&gt; 2021-01-01, 2023-12-01, 2011-06-04, 2022-10-11\n$ Time     &lt;dttm&gt; 2025-07-23 01:20:00, 2025-07-23 02:30:00, 2025-07-23 03:50:00…\n$ DateTime &lt;dttm&gt; 2021-01-01 01:20:00, 2023-12-01 02:30:00, 2011-06-04 03:50:00…\n\ntz(df_date_times_c$DateTime)\n\n[1] \"America/Los_Angeles\"\n\n\nIf you do really want to deal with just the time part of a date-time object, use the hms package.\n\nlibrary(hms)\n\ntimes\n\n[1] \"2025-07-23 01:20:00 PDT\" \"2025-07-23 02:30:00 PDT\"\n[3] \"2025-07-23 03:50:00 PDT\" \"2025-07-23 14:00:00 PDT\"\n\nas_hms(times)\n\n01:20:00\n02:30:00\n03:50:00\n14:00:00",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_02_Dates_and_Times.html#extracting-parts-of-a-date-time-object",
    "href": "code/Day4_02_Dates_and_Times.html#extracting-parts-of-a-date-time-object",
    "title": "\n9  Dates and Times\n",
    "section": "\n9.8 Extracting parts of a date-time object",
    "text": "9.8 Extracting parts of a date-time object\nThe lubridate package has a lot of really useful functions to extract components from dates and times such as month, year, day, etc.\n\ndate1_conv\n\n[1] \"2021-02-03 PST\"\n\n# Year\nyear(date1_conv)\n\n[1] 2021\n\n# Month (as number or name)\nmonth(date1_conv)\n\n[1] 2\n\nmonth(date1_conv, label = TRUE)\n\n[1] Feb\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n# Day\nday(date1_conv)\n\n[1] 3\n\n# Date\ndate(date1_conv)\n\n[1] \"2021-02-03\"\n\n# Day of year\nyday(date1_conv)\n\n[1] 34\n\n# Day of week\nwday(date1_conv, label = TRUE)\n\n[1] Wed\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n9.8.1 Exercise\nNow its your turn to try extracting parts from a date-time object.\nUse the date-time object you created in the last exercise. Try extracting various components from it using the functions in the lubridate package.\nClick below for the answer when you are done!\n\nCode# View the date-time object created in last exercise\nmy_datetime\n\n# Extract the Date\ndate(my_datetime)\n\n# Extract the Year\nyear(my_datetime)\n\n# Extract the Month as label\nmonth(my_datetime, label = TRUE)",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "code/Day4_03_Joins_Binds.html",
    "href": "code/Day4_03_Joins_Binds.html",
    "title": "\n10  Binds and Joins\n",
    "section": "",
    "text": "10.1 Introduction\nSometimes your data analysis project requires more than one data set that need to be combined in order to perform your analysis. This chapter covers two common methods of combining more than one data set in R - binds and joins. We’ll start this lesson covering the differences between these two methods including the situations to best use them in, and then move into the powerful R functions from the dplyr package that help with combining data frames.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Binds and Joins</span>"
    ]
  },
  {
    "objectID": "code/Day4_03_Joins_Binds.html#load-packages",
    "href": "code/Day4_03_Joins_Binds.html#load-packages",
    "title": "\n10  Binds and Joins\n",
    "section": "\n10.2 Load Packages",
    "text": "10.2 Load Packages\nWe’ll start with loading packages\n\nlibrary(tidyverse)\nlibrary(here)",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Binds and Joins</span>"
    ]
  },
  {
    "objectID": "code/Day4_03_Joins_Binds.html#differences-between-binds-and-joins",
    "href": "code/Day4_03_Joins_Binds.html#differences-between-binds-and-joins",
    "title": "\n10  Binds and Joins\n",
    "section": "\n10.3 Differences between Binds and Joins",
    "text": "10.3 Differences between Binds and Joins\nBinding either rows or columns from multiple data frames involves a simple combination of data frames. For example binding two data frames by row, makes a single data frame with more rows:\n\n\n\n\n\n\n\n\nNotice that in this graphic, R recognizes that the two data frames share the same columns and simply appends the rows from Z to X with the proper column assignments.\nIn contrast, binding two data frames by columns performs the perpendicular operation resulting in a single data frame with more columns:\n\n\n\n\n\n\n\n\nBinding data frames by columns combines the data in the order in which they appear, which can create unintended results without you realizing it. It is much safer to use a join when you want to add columns from one data frame to another. For example, here is a simple graphic demonstrating a left join to join two data frames together:\n\n\n\n\nImage Credit https://stat545.stat.ubc.ca/tutorials/tibble_join/\n\n\n\nJoins are a safer method of combining data frame columns because they rely on keys, which are variables used to make the connection between two data frames. In the simple example above, the ID column is the key with which the tables are joined. If this was performed by simply combining columns, you would get much a different and erroneous result. There is a lot more to cover with the concept of keys, but the most important thing to remember is that they are unique for each observation or row in a data set.\n\n\n\n\n\n\nWhen should I use bind vs. join?\n\n\n\nAfter all this information, you may be wondering when is it best to use bind vs. join when combining data frames. Here are some simple guidelines:\n\nWhen you want to add new rows from one data frame to another use bind rows. The columns should (mostly) be the same in the two data sets for this to be effective.\nWhen you want to add new columns from one data frame to another, preferably use a join if they share a common key. Only use bind columns when you have no other option and pay close attention to the order in which the rows appear in the two data sets.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Binds and Joins</span>"
    ]
  },
  {
    "objectID": "code/Day4_03_Joins_Binds.html#bind-rows",
    "href": "code/Day4_03_Joins_Binds.html#bind-rows",
    "title": "\n10  Binds and Joins\n",
    "section": "\n10.4 Bind rows",
    "text": "10.4 Bind rows\nNow that we have a better understanding of what it means to bind the rows of multiple data frames, let’s learn about how to actually perform this operation. The bind_rows function within the dplyr package is the easiest way I’ve found to accomplish this. Let’s use the EMP water quality data set we’ve been working with to demonstrate how to bind rows. To start, we need a second data frame to bind to it. We’ll import both the 2020-2022 EMP data we’ve been working with and some additional EMP water quality data collected in 2019 at the same two stations - P8 and D7 - to be combined to the 2020-2022 data.\n\ndf_wq &lt;- read_csv(here(\"data/WQ_P8D7.csv\"))\ndf_wq_2019 &lt;- read_csv(here(\"data/WQ_2019.csv\"))\n\nLet’s take a look at the structure and columns of the two data frames before binding them.\n\n# 2020-2022 data:\nglimpse(df_wq)\n\nRows: 62\nColumns: 20\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2020-01-16, 2020-01-22, 2020-02-14, 2020-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.64, 0.67, 1.46, 2.15, 1.40, 1.89, 4.73, 1.74, 6.4…\n$ Pheophytin         &lt;dbl&gt; 0.50, 0.87, 0.69, 0.50, 0.56, 1.13, 1.25, 0.89, 0.8…\n$ TotAlkalinity      &lt;dbl&gt; 98.0, 82.0, 81.0, 86.0, 80.0, 93.0, 59.0, 78.0, 63.…\n$ DissAmmonia        &lt;dbl&gt; 0.150, 0.210, 0.250, 0.140, 0.110, 0.220, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 2.800, 0.490, 1.700, 0.480, 1.600, 0.380, 1.070, 0.…\n$ DOC                &lt;dbl&gt; 3.90, 0.27, 2.80, 0.39, 2.00, 0.19, 2.80, 1.20, 3.1…\n$ TOC                &lt;dbl&gt; 4.10, 0.32, 2.50, 0.41, 2.10, 0.20, 2.80, 1.20, 3.1…\n$ DON                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0.30, 0.20, 0.30, 0.10, 0.5…\n$ TotPhos            &lt;dbl&gt; 0.310, 0.082, 0.130, 0.130, 0.190, 0.100, 0.188, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.200, 0.071, 0.130, 0.065, 0.140, 0.082, 0.177, 0.…\n$ TDS                &lt;dbl&gt; 380, 9500, 340, 5800, 290, 8700, 280, 7760, 227, 11…\n$ TSS                &lt;dbl&gt; 8.9, 38.0, 2.2, 18.0, 1.4, 28.0, 6.6, 35.6, 5.3, 23…\n$ TKN                &lt;dbl&gt; 0.520, 0.480, 0.430, 0.250, 0.400, 0.200, 0.400, 0.…\n$ Depth              &lt;dbl&gt; 28.9, 18.8, 39.0, 7.1, 39.0, 7.2, 37.1, 5.2, 36.7, …\n$ Secchi             &lt;dbl&gt; 116, 30, 212, 52, 340, 48, 100, 40, 160, 44, 120, 6…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 3, 2, 3, 2, 4, 2, 3, 2, 2, 1, 1, …\n$ SpCndSurface       &lt;dbl&gt; 667, 15532, 647, 11369, 530, 16257, 503, 12946, 404…\n$ WTSurface          &lt;dbl&gt; 9.67, 9.97, 11.09, 12.51, 13.97, 13.81, 23.46, 21.1…\n\nnames(df_wq)\n\n [1] \"Station\"            \"Date\"               \"Chla\"              \n [4] \"Pheophytin\"         \"TotAlkalinity\"      \"DissAmmonia\"       \n [7] \"DissNitrateNitrite\" \"DOC\"                \"TOC\"               \n[10] \"DON\"                \"TotPhos\"            \"DissOrthophos\"     \n[13] \"TDS\"                \"TSS\"                \"TKN\"               \n[16] \"Depth\"              \"Secchi\"             \"Microcystis\"       \n[19] \"SpCndSurface\"       \"WTSurface\"         \n\n# 2019 data:\nglimpse(df_wq_2019)\n\nRows: 24\nColumns: 20\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2019-01-15, 2019-01-17, 2019-02-15, 2019-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.84, 0.88, 2.29, 1.92, 2.60, 2.53, 1.59, 2.81, 1.0…\n$ Pheophytin         &lt;dbl&gt; 1.21, 0.64, 1.03, 2.32, 1.42, 1.35, 1.25, 1.19, 0.5…\n$ TotAlkalinity      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ DissAmmonia        &lt;dbl&gt; 0.200, 0.214, 0.090, 0.070, 0.050, 0.050, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 3.600, 0.570, 1.030, 0.370, 0.540, 0.300, 0.420, 0.…\n$ DOC                &lt;dbl&gt; 6.00, 1.70, 6.10, 4.20, 3.60, 2.50, 2.60, 2.30, 2.2…\n$ TOC                &lt;dbl&gt; 6.00, 1.70, 5.60, 4.20, 3.60, 2.50, 2.60, 2.10, 2.2…\n$ DON                &lt;dbl&gt; 0.50, 0.10, 0.50, 0.30, 0.38, 0.20, 0.20, 0.16, 0.2…\n$ TotPhos            &lt;dbl&gt; 0.370, 0.094, 0.153, 0.180, 0.070, 0.080, 0.100, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.329, 0.092, 0.120, 0.071, 0.075, 0.065, 0.072, 0.…\n$ TDS                &lt;dbl&gt; 491, 8530, 160, 133, 112, 108, 109, 94, 118, 150, 5…\n$ TSS                &lt;dbl&gt; 12.0, 24.0, 17.0, 108.0, 6.0, 36.0, 27.0, 22.0, 4.0…\n$ TKN                &lt;dbl&gt; 0.800, 0.380, 0.700, 0.600, 0.400, 0.400, 0.300, 0.…\n$ Depth              &lt;dbl&gt; 38.7, 8.2, 17.0, 8.0, 36.7, 8.2, 39.0, 7.0, 37.8, 6…\n$ Secchi             &lt;dbl&gt; 100, 52, 44, 16, 76, 28, 92, 28, 100, 40, 80, 44, 1…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, …\n$ SpCndSurface       &lt;dbl&gt; 878, 14498, 276, 228, 194, 189, 195, 168, 214, 276,…\n$ WTSurface          &lt;dbl&gt; 10.03, 10.37, 10.87, 9.19, 12.12, 12.70, 14.26, 14.…\n\nnames(df_wq_2019)\n\n [1] \"Station\"            \"Date\"               \"Chla\"              \n [4] \"Pheophytin\"         \"TotAlkalinity\"      \"DissAmmonia\"       \n [7] \"DissNitrateNitrite\" \"DOC\"                \"TOC\"               \n[10] \"DON\"                \"TotPhos\"            \"DissOrthophos\"     \n[13] \"TDS\"                \"TSS\"                \"TKN\"               \n[16] \"Depth\"              \"Secchi\"             \"Microcystis\"       \n[19] \"SpCndSurface\"       \"WTSurface\"         \n\n\nYou can see that all the column names and their types (classes) are the same which makes combining them by row a simple operation. We’ll use the bind_rows function to add the 2019 data to the 2020-2022 data and assign it to a new data frame object.\n\ndf_wq_2019_2022 &lt;- bind_rows(df_wq_2019, df_wq)\ndf_wq_2019_2022\n\n# A tibble: 86 × 20\n   Station Date        Chla Pheophytin TotAlkalinity DissAmmonia\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 P8      2019-01-15  0.84       1.21            NA       0.2  \n 2 D7      2019-01-17  0.88       0.64            NA       0.214\n 3 P8      2019-02-15  2.29       1.03            NA       0.09 \n 4 D7      2019-02-20  1.92       2.32            NA       0.07 \n 5 P8      2019-03-14  2.6        1.42            NA       0.05 \n 6 D7      2019-03-18  2.53       1.35            NA       0.05 \n 7 P8      2019-04-12  1.59       1.25            NA       0.05 \n 8 D7      2019-04-16  2.81       1.19            NA       0.05 \n 9 P8      2019-05-13  1.03       0.5             NA       0.05 \n10 D7      2019-05-15  2.1        0.69            NA       0.05 \n# ℹ 76 more rows\n# ℹ 14 more variables: DissNitrateNitrite &lt;dbl&gt;, DOC &lt;dbl&gt;, TOC &lt;dbl&gt;,\n#   DON &lt;dbl&gt;, TotPhos &lt;dbl&gt;, DissOrthophos &lt;dbl&gt;, TDS &lt;dbl&gt;, TSS &lt;dbl&gt;,\n#   TKN &lt;dbl&gt;, Depth &lt;dbl&gt;, Secchi &lt;dbl&gt;, Microcystis &lt;dbl&gt;,\n#   SpCndSurface &lt;dbl&gt;, WTSurface &lt;dbl&gt;\n\nrange(df_wq_2019_2022$Date)\n\n[1] \"2019-01-15\" \"2022-12-19\"\n\n\nThe new data frame now has EMP water quality data from 2019-2022.\n\n\n\n\n\n\nNote\n\n\n\nYou may be wondering what would happen if the two data frames had a few columns with different names. bind_rows would still combine the two data frames but it would preserve separate columns for the ones with different names.\nHowever, if the two data frames had columns with the same name but with different types (classes - numeric, character, etc.), bind_rows will not work and give you an error. It is always best practice to check this by using glimpse or str on the two data frames before proceeding with binding their rows.\n\n\n\n10.4.1 Exercise\nNow its your turn to try out combining two data sets by binding rows.\nAdd water quality data collected from 2020-2022 at C3A to the original data frame (df_wq) with data from P8 and D7. Assign this data frame to a new object to be used later. The C3A data is in the “WQ_C3A_2020_2022.csv” file.\nClick below for the answer when you are done!\n\nCode# Import EMP water quality data for C3A\ndf_wq_c3a &lt;- read_csv(here(\"data/WQ_C3A_2020_2022.csv\"))\n\n# Check the structure and column names of the two data frames\nglimpse(df_wq_c3a)\nnames(df_wq_c3a)\n\nglimpse(df_wq)\nnames(df_wq)\n\n# Add C3A to original data frame with P8 and D7\ndf_wq_p8_d7_c3a &lt;- bind_rows(df_wq, df_wq_c3a)\n\n# Take a look at the combined data set\ndf_wq_p8_d7_c3a %&gt;% arrange(Date, Station)",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Binds and Joins</span>"
    ]
  },
  {
    "objectID": "code/Day4_03_Joins_Binds.html#join-basics",
    "href": "code/Day4_03_Joins_Binds.html#join-basics",
    "title": "\n10  Binds and Joins\n",
    "section": "\n10.5 Join basics",
    "text": "10.5 Join basics\nThe dplyr package provides various methods to join data frames together:\n\n\n\n\nImage Credit https://r4ds.hadley.nz/joins\n\n\n\nIn the section above, we touched on the concept of keys and their importance with joins. Here is an illustration that helps explain keys further and how they assist with various types of joins:\n\n\n\n\nImage Credit https://r4ds.hadley.nz/joins\n\n\n\nIn this figure the join keys are color-coded and the matching pairs share the same color. Keys 1 and 2 match between the two data frames and are joined together in the resulting data frame. The differences between the join types lies in how they handle keys that don’t have a match. In short, a left_join keeps all rows in the first data frame, a right_join keeps all observations in the second data frame, and a full_join keeps all rows from both data frames.",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Binds and Joins</span>"
    ]
  },
  {
    "objectID": "code/Day4_03_Joins_Binds.html#left-join",
    "href": "code/Day4_03_Joins_Binds.html#left-join",
    "title": "\n10  Binds and Joins\n",
    "section": "\n10.6 Left join",
    "text": "10.6 Left join\nWhile all these joining methods can be useful, we’ll focus on the most commonly-used joining method - left_join. We’ll use a left_join to add Delta inflow and outflow data from DayFlow to the 2019-2022 EMP water quality data. First, we’ll need to import the DayFlow data:\n\n# Import Delta inflow and outflow data\ndf_dayflow &lt;- read_csv(here(\"data/Dayflow_2019_2023.csv\"))\n\nLet’s take a look at the structure of the two data frames before joining them.\n\n# 2019-2022 EMP water quality data:\nglimpse(df_wq_2019_2022)\n\nRows: 86\nColumns: 20\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2019-01-15, 2019-01-17, 2019-02-15, 2019-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.84, 0.88, 2.29, 1.92, 2.60, 2.53, 1.59, 2.81, 1.0…\n$ Pheophytin         &lt;dbl&gt; 1.21, 0.64, 1.03, 2.32, 1.42, 1.35, 1.25, 1.19, 0.5…\n$ TotAlkalinity      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ DissAmmonia        &lt;dbl&gt; 0.200, 0.214, 0.090, 0.070, 0.050, 0.050, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 3.600, 0.570, 1.030, 0.370, 0.540, 0.300, 0.420, 0.…\n$ DOC                &lt;dbl&gt; 6.00, 1.70, 6.10, 4.20, 3.60, 2.50, 2.60, 2.30, 2.2…\n$ TOC                &lt;dbl&gt; 6.00, 1.70, 5.60, 4.20, 3.60, 2.50, 2.60, 2.10, 2.2…\n$ DON                &lt;dbl&gt; 0.50, 0.10, 0.50, 0.30, 0.38, 0.20, 0.20, 0.16, 0.2…\n$ TotPhos            &lt;dbl&gt; 0.370, 0.094, 0.153, 0.180, 0.070, 0.080, 0.100, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.329, 0.092, 0.120, 0.071, 0.075, 0.065, 0.072, 0.…\n$ TDS                &lt;dbl&gt; 491, 8530, 160, 133, 112, 108, 109, 94, 118, 150, 5…\n$ TSS                &lt;dbl&gt; 12.0, 24.0, 17.0, 108.0, 6.0, 36.0, 27.0, 22.0, 4.0…\n$ TKN                &lt;dbl&gt; 0.800, 0.380, 0.700, 0.600, 0.400, 0.400, 0.300, 0.…\n$ Depth              &lt;dbl&gt; 38.7, 8.2, 17.0, 8.0, 36.7, 8.2, 39.0, 7.0, 37.8, 6…\n$ Secchi             &lt;dbl&gt; 100, 52, 44, 16, 76, 28, 92, 28, 100, 40, 80, 44, 1…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, …\n$ SpCndSurface       &lt;dbl&gt; 878, 14498, 276, 228, 194, 189, 195, 168, 214, 276,…\n$ WTSurface          &lt;dbl&gt; 10.03, 10.37, 10.87, 9.19, 12.12, 12.70, 14.26, 14.…\n\n# DayFlow data:\nglimpse(df_dayflow)\n\nRows: 1,734\nColumns: 3\n$ Date    &lt;date&gt; 2019-01-01, 2019-01-02, 2019-01-03, 2019-01-04, 2019-01-05, 2…\n$ Inflow  &lt;dbl&gt; 14763, 14049, 13830, 13435, 13354, 15286, 19296, 25543, 30357,…\n$ Outflow &lt;dbl&gt; 7335, 6445, 6425, 6273, 7534, 17086, 21143, 27350, 33043, 3447…\n\n\nBoth data frames share the Date column which will be our join key. Note that the Date columns are “date” class in both data sets. This is important - if they were different data classes, R wouldn’t know how to match them together. Also note that the DayFlow data set has observations for every day between 1/1/2019 to 9/30/2023, while the EMP water quality data frame only has a few observations per month. Because we are interested in adding the DayFlow values to the water quality data set, we’ll join them together with a left_join with the water quality data frame as the first data frame. This will only keep the rows within the water quality data set and drop the unmatching rows from the DayFlow data set.\n\n# Join Delta flow data to 2019-2022 WQ data\ndf_wq_flow &lt;- left_join(df_wq_2019_2022, df_dayflow)\n\nJoining with `by = join_by(Date)`\n\nglimpse(df_wq_flow)\n\nRows: 86\nColumns: 22\n$ Station            &lt;chr&gt; \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8\", \"D7\", \"P8…\n$ Date               &lt;date&gt; 2019-01-15, 2019-01-17, 2019-02-15, 2019-02-20, 20…\n$ Chla               &lt;dbl&gt; 0.84, 0.88, 2.29, 1.92, 2.60, 2.53, 1.59, 2.81, 1.0…\n$ Pheophytin         &lt;dbl&gt; 1.21, 0.64, 1.03, 2.32, 1.42, 1.35, 1.25, 1.19, 0.5…\n$ TotAlkalinity      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ DissAmmonia        &lt;dbl&gt; 0.200, 0.214, 0.090, 0.070, 0.050, 0.050, 0.050, 0.…\n$ DissNitrateNitrite &lt;dbl&gt; 3.600, 0.570, 1.030, 0.370, 0.540, 0.300, 0.420, 0.…\n$ DOC                &lt;dbl&gt; 6.00, 1.70, 6.10, 4.20, 3.60, 2.50, 2.60, 2.30, 2.2…\n$ TOC                &lt;dbl&gt; 6.00, 1.70, 5.60, 4.20, 3.60, 2.50, 2.60, 2.10, 2.2…\n$ DON                &lt;dbl&gt; 0.50, 0.10, 0.50, 0.30, 0.38, 0.20, 0.20, 0.16, 0.2…\n$ TotPhos            &lt;dbl&gt; 0.370, 0.094, 0.153, 0.180, 0.070, 0.080, 0.100, 0.…\n$ DissOrthophos      &lt;dbl&gt; 0.329, 0.092, 0.120, 0.071, 0.075, 0.065, 0.072, 0.…\n$ TDS                &lt;dbl&gt; 491, 8530, 160, 133, 112, 108, 109, 94, 118, 150, 5…\n$ TSS                &lt;dbl&gt; 12.0, 24.0, 17.0, 108.0, 6.0, 36.0, 27.0, 22.0, 4.0…\n$ TKN                &lt;dbl&gt; 0.800, 0.380, 0.700, 0.600, 0.400, 0.400, 0.300, 0.…\n$ Depth              &lt;dbl&gt; 38.7, 8.2, 17.0, 8.0, 36.7, 8.2, 39.0, 7.0, 37.8, 6…\n$ Secchi             &lt;dbl&gt; 100, 52, 44, 16, 76, 28, 92, 28, 100, 40, 80, 44, 1…\n$ Microcystis        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, …\n$ SpCndSurface       &lt;dbl&gt; 878, 14498, 276, 228, 194, 189, 195, 168, 214, 276,…\n$ WTSurface          &lt;dbl&gt; 10.03, 10.37, 10.87, 9.19, 12.12, 12.70, 14.26, 14.…\n$ Inflow             &lt;dbl&gt; 27349, 42029, 129533, 107109, 129444, 105932, 10488…\n$ Outflow            &lt;dbl&gt; 21963, 42727, 130451, 95024, 121805, 97089, 100003,…\n\n\nThe resulting data frame has the same number of rows as the df_wq_2019_2022 water quality data set with the addition of two columns - Inflow and Outflow - which are the joined values from the DayFlow data set. Note that R gave an informative message regarding which columns it performed the join by, which in this case is the Date column.\n\n10.6.1 Exercise\nNow its your turn to try out joining two data sets.\nUse a left_join to add weather observation data to the 2020-2022 water quality data collected at P8, D7, and C3A (the data frame you created in the bind rows exercise above). The weather observation data is in the “Weather_Obs_P8_D7_C3A_2020_2022.csv” file.\nClick below for the answer when you are done!\n\nCode# Import the weather observation data\ndf_weather &lt;- read_csv(here(\"data/Weather_Obs_P8_D7_C3A_2020_2022.csv\"))\n\n# Check the structure of the two data frames to be joined\nglimpse(df_wq_p8_d7_c3a)\nglimpse(df_weather)\n\n# Note that the two data frames share the \"Station\" and \"Date\" columns.\n# These will be the two columns that the data frames will be joined by.\n# In combination they form a unique combination, which is known as a \n# compound key.\n# Join the weather data to the water quality data\ndf_wq_weather &lt;- left_join(df_wq_p8_d7_c3a, df_weather)\n\n# Take a look at the joined data set\nglimpse(df_wq_weather)",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Binds and Joins</span>"
    ]
  },
  {
    "objectID": "code/Day4_04_Final_Exercise.html",
    "href": "code/Day4_04_Final_Exercise.html",
    "title": "11  Final Course Exercise",
    "section": "",
    "text": "In this final course exercise, we will combine many of the concepts we’ve learned in this class together to explore a couple new data sets. The first data set we’ll be using contains continuous water quality data for various parameters collected at the Toe Drain at Lisbon Weir (LIS) site from 2014-2019. This data file contains measurements collected every 15 minutes with a YSI multi-parameter sonde. The data file “rtm_wq_lis_2014_2018.rds” has the following columns:\n\nDateTime - Date and time of measurement in PST\nWaterTemp - Water temperature in degrees Celsius\nTurbidity - Turbidity (water clarity) in FNU\nSpCnd - Specific conductance in uS/cm\npH - pH (unitless)\nDO - Dissolved oxygen in mg/L\nChla - Chlorophyll-a fluorescence in ug/L\ncolumns ending with _Qual - Quality code columns for each water quality parameter\n\n\nStart with importing the continuous water quality data for LIS. The file should be at the following path in your R project: “data/rtm_wq_lis_2014_2018.rds”\n\n\n\n\n\n\n\nA note about .rds and .RData files\n\n\n\n.rds and .RData files are file extensions specific to the R programming language. Both file types offer compression to reduce storage space on your computer.\n.rds files store a single R object, typically a data frame, while .RData files contain multiple R objects.\nIn this exercise, we are importing a .rds file. You can use either the readRDS or read_rds functions to import it into your R session.\n\n\n\nAfter importing the LIS data, take a look at its structure to make sure all data read in properly.\nNotice that the DateTime column was imported as a character data class. It will need to be converted to a date-time class in order to proceed with the rest of this exercise. Do so now.\nAfter cleaning up the DateTime column in your data set, create a time-series plot of one of the water quality parameters collected at LIS.\nIt looks like we have data from 2014-2018. We are only interested in the 2015-2016 data, so create a new data frame with just the LIS data for those two years.\nNow, re-create the time-series plot you made above for years 2015-2016.\nTo check the 2015-2016 data for outliers or erroneous values, plot all continuous water quality parameters (WaterTemp, Turbidity, SpCnd, pH, DO, Chla) in time-series plots using facet_wrap to put each parameter in its own subplot.\nHINT: You will need to pivot the data frame to the long format before plotting the data in facets. The long data frame will have three columns: DateTime, a column identifying the water quality parameter, and another for the values. Also, before pivoting the data, you will need to remove the columns ending with _Qual.\nSome of the parameters are difficult to see clearly, so re-create the plot above giving each facet a different y-axis range.\nBoxplots are also nice to use to look for potential outliers in the data, so make a box and whisker plot of the 2015-2016 data using year as the categorical variable and using facet_wrap to put each parameter in its own subplot. Give each facet a different y-axis range.\nHINT: You will need to use the data frame in the long format to create this plot. Also, be sure to convert year to a factor to plot it as a categorical variable.\nThe box and whisker plots show that there may be some possible outliers in some of the parameters, but for the most part, the data looks pretty clean in the time-series plots, so we will use it as is. It turns out that you are only interested in the chlorophyll (Chla) data for 2015-2016. Averaging this data will help smooth out any potential outliers in the data. Calculate the daily average chlorophyll values for 2015-2016 in a new data frame object to be used later.\nHINT: Use the date function from the lubridate package to extract the date from the DateTime column. Also, watch out for NA values in Chla.\nNow, make a time-series plot of the 2015-2016 daily average chlorophyll data. Assign this plot to a new object so it can be used later.\nThis plot is interesting, let’s see how daily average chlorophyll values from a nearby station in the Sacramento River (SRH) compares. Import the 2014-2018 SRH daily average chlorophyll data. The data file is called “dv_chla_srh_2014_2018.csv”. This data file has the following columns:\n\nStation - Identifies station where data was collected. In this case “SRH”.\nDate - Date of measurement\nChla_mean - Daily average chlorophyll-a fluorescence in ug/L\n\nAgain, we are only interested in the 2015-2016 data from SRH, so create a new data frame with just the 2015-2016 data.\nBind the daily average chlorophyll data from SRH to the daily averages from LIS so they can be plotted together.\nHINT: Watch out for possibly different column names. Also, the LIS data needs a new column for Station so it can be differentiated from SRH.\nNow, create time-series plots of the daily average chlorophyll values for LIS and SRH on the same plot with different colors for the stations.\nIt turns out your supervisor really likes this plot, but would like to put it in a report, so they want it to have some nicer formatting. Remake this plot with the following customizations:\n\nMake each station have a different point shape. You can choose from any of these options.\nChange the size of the points to 2 for both stations.\nChange the color of the time-series points and lines so LIS is a shade of blue and SRH is a shade of orange. You can use this color chart to help you choose colors.\nChange the plot theme to theme_bw.\nGive the plot an informative title and descriptive axis labels.\n\nBe sure to assign this plot to a new object so it can be exported as a png file later.\nExport the figure for your supervisor’s report as a png file with a width of 6 inches and height of 4 inches and a dpi of 300. You also want the 2015-2016 daily average chlorophyll data for LIS and SRH, so export that as a csv file.\nAfter thinking about the results more, you notice that LIS varies throughout the summer months in 2016. Could this have something to do with water flow at LIS? It turns out we in luck because water flow is also collected at LIS. Import 2014-2018 daily average water flow (cfs) collected at LIS. The data file is called “dv_flow_lis_2014_2018.csv”. This data file has the following columns:\n\nDate - Date of measurement\nFlow - Daily average water flow in cfs\n\nAgain, we are only interested in the 2015-2016 flow data from LIS, so create a new data frame with just the 2015-2016 data.\nCreate a time-series plot of the 2015-2016 daily average flow data for LIS. Assign this plot to a new object so it can be used later.\nCombine the 2015-2016 chlorophyll and water flow plots for LIS into one plot arranged vertically using the patchwork package to take a look at how chlorophyll and flow vary across time.\nThere does seem to be some relationship between chlorophyll and flow particularly during 2016 at LIS. We’ll use a scatterplot of chlorophyll and flow data to take a look at this possible relationship in a different way. To accomplish this, first you will need to join the daily average flow data for LIS to the daily average chlorophyll data and filter it to just 2016.\nNext, create the scatterplot of the 2016 LIS data with daily average flow on the x-axis and daily average chlorophyll on the y-axis.\nBonus: Add a trend line to this scatterplot using a linear model (LM).\n\nIf you have some extra time, explore other aspects of the data on your own.\nClick below for the answer when you are done!\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(here)\n\n# 1) Import LIS water quality data\ndf_wq_lis &lt;- readRDS(here(\"data/rtm_wq_lis_2014_2018.rds\"))\n\n# 2) Take a look at the structure of the imported data frame\nglimpse(df_wq_lis)\n\n# 3) Convert DateTime column from character to date-time class\ndf_wq_lis_c &lt;- df_wq_lis %&gt;% mutate(DateTime = ymd_hms(DateTime, tz = \"Etc/GMT+8\"))\n\n# 4) Create a time-series plot of the water temperature data\ndf_wq_lis_c %&gt;% \n  ggplot(aes(x = DateTime, y = WaterTemp)) +\n  geom_point()\n\n# 5) Create a new data frame for LIS water quality data for just 2015-2016\ndf_wq_lis_1516 &lt;- df_wq_lis_c %&gt;% filter(year(DateTime) %in% 2015:2016)\n\n# 6) Re-create the time-series plot of water temperature for years 2015-2016\ndf_wq_lis_1516 %&gt;% \n  ggplot(aes(x = DateTime, y = WaterTemp)) +\n  geom_point()\n\n# 7) Plot all continuous water quality parameters in time-series plots using\n  # facet_wrap to put each parameter in its own subplot.\n\n# Pivot the data frame to the long format before plotting the data\nlis_wq_param &lt;- c(\"WaterTemp\", \"Turbidity\", \"SpCnd\", \"DO\", \"pH\", \"Chla\")\n\ndf_wq_lis_1516_l &lt;- df_wq_lis_1516 %&gt;% \n  select(DateTime, all_of(lis_wq_param)) %&gt;% \n  pivot_longer(\n    cols = all_of(lis_wq_param),\n    names_to = \"Parameter\",\n    values_to = \"Value\"\n  ) %&gt;% \n  drop_na(Value)\n\n# Create plot\ndf_wq_lis_1516_l %&gt;% \n  ggplot(aes(x = DateTime, y = Value)) +\n  geom_point() +\n  facet_wrap(vars(Parameter))\n\n# 8) Re-create the plot above giving each facet a different y-axis range\ndf_wq_lis_1516_l %&gt;% \n  ggplot(aes(x = DateTime, y = Value)) +\n  geom_point() +\n  facet_wrap(vars(Parameter), scales = \"free_y\")\n\n# 9) Create a box and whisker plot of the 2015-2016 data using year as the\n  # categorical variable and using facet_wrap to put each parameter in its own\n  # subplot. Give each facet a different y-axis range.\ndf_wq_lis_1516_l %&gt;% \n  # Create a categorical (factor) column for Year\n  mutate(Year = factor(year(DateTime))) %&gt;% \n  ggplot(aes(x = Year, y = Value)) +\n  geom_boxplot() +\n  facet_wrap(vars(Parameter), scales = \"free_y\")\n\n# 10) Calculate daily average chlorophyll values for 2015-2016 in a new data\n  # frame object to be used later\ndf_chla_lis_1516 &lt;- df_wq_lis_1516 %&gt;% \n  # Create a column for Date\n  mutate(Date = date(DateTime)) %&gt;% \n  group_by(Date) %&gt;% \n  # Use na.rm = TRUE to ignore NA values in Chla\n  summarize(Chla = mean(Chla, na.rm = TRUE))\n\n# 11) Create a time-series plot of the 2015-2016 daily average chlorophyll data.\n  # Assign this plot to a new object so it can be used later\nplt_chla_lis_1516 &lt;- df_chla_lis_1516 %&gt;% \n  ggplot(aes(x = Date, y = Chla)) +\n  geom_point() +\n  geom_line()\n\n# Print out plot\nplt_chla_lis_1516\n\n# 12) Import SRH daily average chlorophyll data\ndf_chla_srh &lt;- read_csv(here(\"data/dv_chla_srh_2014_2018.csv\"))\n\n# 13) Create a new data frame for SRH daily average chlorophyll data for just 2015-2016\ndf_chla_srh_1516 &lt;- df_chla_srh %&gt;% filter(year(Date) %in% 2015:2016)\n\n# 14) Bind the daily average chlorophyll data from SRH to the daily averages from LIS\n\n# Rename Chla column in SRH data frame so it matches the LIS data frame\ndf_chla_srh_1516_c &lt;- df_chla_srh_1516 %&gt;% rename(Chla = Chla_mean)\n\ndf_chla_1516 &lt;- df_chla_lis_1516 %&gt;% \n  # Create a new column for Station in the LIS data frame\n  mutate(Station = \"LIS\", .before = Date) %&gt;% \n  bind_rows(df_chla_srh_1516_c)\n\n# 15) Create time-series plots of the daily average chlorophyll values for LIS\n  # and SRH on the same plot with different colors for the stations\ndf_chla_1516 %&gt;% \n  ggplot(aes(x = Date, y = Chla, color = Station)) +\n  geom_point() +\n  geom_line()\n\n# 16) Remake the plot above with the specified customizations. Assign this plot\n  # to a new object so it can be exported as a png file later\nplt_chla_1516 &lt;- df_chla_1516 %&gt;% \n  ggplot(aes(x = Date, y = Chla, color = Station)) +\n  geom_point(aes(shape = Station), size = 2) +\n  geom_line() +\n  theme_bw() +\n  scale_color_manual(values = c(LIS = \"midnightblue\", SRH = \"darkorange3\")) +\n  scale_shape_manual(values = c(15, 17)) +\n  labs(\n    title = \"Chlorophyll Comparision between LIS and SRH in 2015-2016\",\n    y = \"Daily Average Chlorophyll (ug/L)\"\n  )\n\n# Print out plot\nplt_chla_1516\n\n# 17) Export the figure for your supervisor's report as a png file. Also export\n  # the 2015-2016 daily average chlorophyll data for LIS and SHR as a csv file.\n\n# Export plot\nggsave(\n  plot = plt_chla_1516,\n  filename = \"chla_lis_srh_comp.png\",\n  width = 6, \n  height = 4, \n  units = \"in\",\n  dpi = 300\n)\n\n# Export daily average chlorophyll data\ndf_chla_1516 %&gt;% write_csv(\"dv_chla_lis_srh_2015_2016.csv\")\n\n# 18) Import 2014-2018 daily average water flow (cfs) collected at LIS\ndf_q_lis &lt;- read_csv(here(\"data/dv_flow_lis_2014_2018.csv\"))\n\n# 19) Create a new data frame for LIS daily average water flow for just 2015-2016\ndf_q_lis_1516 &lt;- df_q_lis %&gt;% filter(year(Date) %in% 2015:2016)\n\n# 20) Create a time-series plot of the 2015-2016 daily average flow data for\n  # LIS. Assign this plot to a new object so it can be used later\nplt_q_lis_1516 &lt;- df_q_lis_1516 %&gt;% \n  ggplot(aes(x = Date, y = Flow)) + \n  geom_point() +\n  geom_line()\n\n# Print out plot\nplt_q_lis_1516\n\n# 21) Combine the 2015-2016 chlorophyll and water flow plots for LIS into one\n  # plot arranged vertically using the patchwork package\nplt_chla_lis_1516 / plt_q_lis_1516\n\n# 22) Join the daily average flow data for LIS to the daily average chlorophyll\n  # data and filter it to just 2016\ndf_chla_q_lis_16 &lt;- df_chla_lis_1516 %&gt;% \n  # Join flow data to chlorophyll data\n  left_join(df_q_lis_1516, by = join_by(Date)) %&gt;% \n  # Filter to just 2016\n  filter(year(Date) == 2016) %&gt;% \n  # Remove rows with NA values in Flow column\n  drop_na(Flow)\n\n# 23) Create a scatterplot of the 2016 LIS data with daily average flow on the\n  # x-axis and daily average chlorophyll on the y-axis\ndf_chla_q_lis_16 %&gt;% \n  ggplot(aes(x = Flow, y = Chla)) +\n  geom_point()\n\n# 24) Add a trend line to the scatterplot above using a linear model (LM)\ndf_chla_q_lis_16 %&gt;% \n  ggplot(aes(x = Flow, y = Chla)) +\n  geom_point() +\n  geom_smooth(method = lm, formula = \"y ~ x\")",
    "crumbs": [
      "Day 4 - Binds, Joins, Dates and Times",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Final Course Exercise</span>"
    ]
  },
  {
    "objectID": "code/MarkdownFiles.html",
    "href": "code/MarkdownFiles.html",
    "title": "Appendix A — Bonus lesson - Markdown Files",
    "section": "",
    "text": "There are two ways to write and save R code: scripts (.R) and markdown files (.Rmd or .qmd).\nScripts are very bare bones. You cannot add “normal” text or formatting to them and their output is not saved anywhere in the file. This is the ideal format for purely utilitarian code. They are equivalent to writing a document in Notepad.\nMarkdown files are formatted files that allow you to present code in a reader-friendly design. Unlike scripts, you can add “normal” text and formatting, and the code output can be saved and displayed directly in the file. You can even add code from multiple different languages! This is the ideal format when visually examining your output matters. They are equivalent to writing a document in Word.\n\n\n\n\n\n\nMarkdown\n\n\n\nWhy are they called “Markdown files”? Markdown is simply a language for creating formatted text (like html). For practical use, all that matters is markdown files preserve formatting (and therefore allow text, images, etc. to be in the file) while scripts do not.\n\n\n\n\n\n\n\n\n.Rmd vs. .qmd\n\n\n\nNewer versions of R have three versions of markdown files: R markdown, R notebook, and Quarto document.\nR markdown and R notebook files are functionally the same from a coding perspective (hence why they have the same extension, .Rmd). The only practical difference is you can preview R notebooks without knitting them; this can be useful if you want to quickly view the output.\nQuarto documents (.qmd) differ from R markdown in that they’re more general; unlike R markdown, which was primarily built with the R language in mind, Quarto is language agnostic. However, aside from some syntax differences, it’s practically the same as a .Rmd file.\nYou can use either format for your code. Quarto is newer and has less documentation currently, but it will likely have more support in the long run, and is preferable for some complex use cases.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Bonus lesson - Markdown Files</span>"
    ]
  },
  {
    "objectID": "code/RDefinitions.html",
    "href": "code/RDefinitions.html",
    "title": "Appendix B — Definitions",
    "section": "",
    "text": "Argument- one of possibly several expressions that are passed to a function\nAssignment - Giving a value to a variable. &lt;- is known as the “assignment operator”.\nComment: Text written in a script that is not treated as code to be run, but rather as text that describes what the code is doing. These are usually short notes, beginning with a #\nConsole - Place you run your code. It prints all the inputs and outputs - but then it’s gone!\nComprehensive R Archive Network (CRAN) - A public repository of R packages\nData frame - A two-dimensional data structure for storing tabular data in memory. Rows represent records and columns represent variables.\nEnvironment - A collection of functions, objects, and variables that are available for you to work with.\nFunction - A code block which gathers a sequence of operations into a whole, preserving it for ongoing use by defining a set of tasks that takes zero or more required and optional arguments as inputs and returns expected outputs (return values), if any. Functions enable repeating these defined tasks with one command, known as a function call.\nGUI - “Graphical User Interface” - anything you can point-and-click to get what you want. This is in contrast to the “command line” where you have to write out what you want.\nScript - Text file with code you want to run\nProject - Set of scripts, data, and outputs that are associated with a working directory and are related to each other using an .Proj file.\nPackage - A collection of code, data, and documentation that can be distributed and re-used. Also referred to in some languages as a library or module.\nPipe operator: The %&gt;% used to make the output of one function the input of the next. Piping works particularly well with Tidyverse packages.\nObject - A data set, a variable, plot, or more formally, almost everything in R. If it has a mode, it is an object. Includes data frames, vectors, matrices, arrays, lists and functions.\nTidyverse - A collection of R packages for operating on tabular data in consistent ways.\nVector - A list of items of the same type (e.g., a list of numbers or words).\nWorking directory - The folder that contains all the scripts and data you are working with.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Definitions</span>"
    ]
  },
  {
    "objectID": "code/Resources.html",
    "href": "code/Resources.html",
    "title": "Appendix C — Useful Resources",
    "section": "",
    "text": "R For Data Science - a must-read e-book for how to handle data in R. https://r4ds.hadley.nz\nCheetsheets for popular R packages. https://rstudio.github.io/cheatsheets\nIEP Data Science Project Workteam homepage. https://interagencyecologicalprogram.github.io/DataScience/\nRSeek - Google for R related websites. https://rseek.org/\nChat GPT and other generative AI apps can help you write code. https://chatgpt.com/\nThe R Graph Gallery - get ideas for new types of graphs, and the code to make them. https://r-graph-gallery.com/\nMore information on the tidyverse packages. https://www.tidyverse.org/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Useful Resources</span>"
    ]
  }
]